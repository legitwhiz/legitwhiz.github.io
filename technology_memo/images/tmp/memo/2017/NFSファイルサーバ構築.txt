
1.NFSサーバの設定
NFSサーバを構築するには、ファイルのシェアの設定とNFSサービスを提供するデーモンの設定を行う必要があります。
ファイルのシェアの設定は、Linuxでは /etc/exports で行います。
NFSに関連したデーモンには、まず、ポートマッパ(portmapper)があります。
Linuxの場合、起動時にはportmapかrpc.portmapのどちらかのプロセス名になります。
これは、要求をよこしたクライアントに対し、システム上のNFSサービスへの接続先を伝えるデーモンです。
ポートマッパにアクセス制限をかけないと全く認証されていないクライアントに対してもNFSデーモンの情報を伝えてしまい、NFS経由でシステムに侵入される危険がありますので注意しましょう。
NFSサービスを提供するデーモンには、nfsd、lockd、mountd、rquotad、statd があります。
システムによっては、それぞれ、rpc.nfsd、rpc.lockd、 rpc.mountd、rpc.rquotad、rpc.statdのようなプロセス名で表示されることもあります。
nfsdが大部分の作業を行い、lockdとstatdがファイルロック、mountdが 接続開始時のマウント要求、rquotadがエクスポートされたボリュームのユーザファイルクォータを提供する役割を果たします。
ファイルクォータを 使用しなければ、rquotadを起動する必要はありません。
Tru64の場合にはrpc.nfsdがnfsdとnfsiodとなります。
NFSサービスを提供するにはこれらのデーモンを起動する(あるいは、inetdやxinetdから起動可能なように設定する)必要があります。
デーモンの起動は、(1) portmapper、(2) mountd, nfsd、(3) statd, lockd, rquotadの順で行います。通常は起動スクリプトでこの順番に起動されるように 設定されています。 

2.NFSクライアントの設定
NFSクライアントを設定する場合にもNFSサーバの場合と同様、ポートマッパを起 動しておく必要があります。ファイルのロックを扱いたい場合にはlockd、statd を起動します(クライアント側、サーバ側の両方で起動している必要があります)。
 起動時にマウントされるようにするには /etc/vfstab (Solaris)、 /etc/fstab(Linux, Tru64) を編集します。

3.自動マウントの設定
 自動マウントを行うには設定ファイルの編集とデーモンの起動が必要です。シス テムによってはデフォルトで起動されるようになっています。自動マウントの設 定ファイルは Solarisでは /etc/auto_master、/etc/auto_home、 /etc/auto_direct です。このうち、auto_home、auto_direct は auto_master から読み込むように設定します。Linux では /etc/auto.master、 /etc/auto.misc が設定ファイルになります。
 自動マウントを行うデーモンはautomountdですが、 Linuxは /usr/sbin/automount にあります。

###############################################################################
NFS のアーキテクチャー
NFS が従うコンピューティング・モデルは、クライアント・サーバー・モデルです (図 1 を参照)。サーバーが共有ファイルシステムとストレージを実装し、そこにクライアントが接続します。クライアントは共有ファイルシステムに対するユーザー・インターフェースを実装し、クライアントのローカル・ファイル空間内にこの共有ファイルシステムをマウントします。

図 1. NFS のクライアント・サーバー・アーキテクチャー
+-------------------------------------+
| +--------------+  +---------------+ |
| | File System  |  | NFS Client    | +----------+
| +--------------+  +---------------+ |          |
+-------------------------------------+          |
                                                 |
                                                 |
                                                 |      +-----------+            +----------------------------------+
                                                 |      |           |            | +-------------+   +------------+ |
                                                 +------+  Network  +------------+ |NFS Server   |   |File System | |
                                                 |      +-----------+            | +-------------+   +------------+ |
                                                 |                               +----------------------------------+
+-------------------------------------+          |
| +--------------+   +--------------+ |          |
| | File System  |   | NFS Client   | +----------+
| +--------------+   +--------------+ |
+-------------------------------------+

LinuxR 内で、複数のファイルシステムを 1 つのホスト (CD-ROM 上の ISO (International Organization for Standardization) 9660 やローカル・ハード・ディスク上の ext3fs など) で同時にサポートする手段となるのは、仮想ファイルシステム・スイッチ (VFS) です。VFS はリクエストの対象となっているストレージを判別した後、そのリクエストに対処するために必要なファイルシステムを判別します。そのため、NFS は他のあらゆるファイルシステムと同様、プラガブルになっています。NFS が他のファイルシステムと違う点は唯一、入出力 (I/O) リクエストがローカル側で対処できなければ、リクエストはネットワークを介してリモート側で対処されることです。

NFS を対象にしたリクエストが検出されると、VFS はそのリクエストをカーネル内の NFS インスタンスに渡します。NFS は I/O リクエストを解釈して NFS のプロシージャー (OPEN、ACCESS、CREATE、READ、CLOSE、REMOVE など) に変換します。これらのプロシージャー (特定の NFS RFC に文書化されています) は、NFS プロトコル内での振る舞いを指定します。I/O リクエストから選択されたプロシージャーは RPC (Remote Procedure Call) 層の中で実行されます。RPC はその名前が示唆するように、システム間をまたがるプロシージャー・コールを実行するための手段です。RPC は NFS リクエストとそれに付随する引数をまとめてマーシャリングしてから該当するリモート・システムに送信します。その後、返されるレスポンスの管理および追跡を行い、そのレスポンスをリクエスト側に送信します。

さらに、RPC は XDR (External Data Representation) と呼ばれる重要な相互運用層を組み込みます。XDR によって、NFS に参加するすべてのシステムがデータ型に関して確実に同じ言語で対話することになります。ある特定のアーキテクチャーが行ったリクエストのデータ型の表現方式が、そのリクエストに対処するターゲット・ホストの表現方式と同じであるとは限りません。そのため XDR は、すべてのアーキテクチャーが相互運用してファイルシステムを共有できるように、データ型を共通表現方式 (XDR) に変換します。XDR は、float などの型のビット・フォーマット、そして固定長および可変長の配列などの型のバイト・オーダーを指定しています。XDR は NFS で使用されていることで最もよく知られていますが、1 つの共通したアプリケーション設定で複数のアーキテクチャーを扱う際には必ず役に立つ仕様です。

XDR でデータが共通表現に変換されたリクエストは、ネットワークを介して転送されます。その際のトランスポート層プロトコルとしては、初期の NFS では UDP (Universal Datagram Protocol) を使用していましたが、現在は信頼性を高めるために TCP が共通して使用されています。

サーバー側でも、NFS は同じように動作します。リクエストは (データ型をサーバーのアーキテクチャーに変換するため) RPC/XDR によりネットワーク・スタックに送られ、NFS サーバーに到達します。NFS サーバーの役目は、リクエストに対処することです。リクエストが NFS デーモンに渡されると、NFS デーモンはリクエストに必要なターゲット・ファイルシステム・ツリーを特定します。ローカル・ストレージ内の該当するファイルシステムにアクセスするために、再び VFS が使用されます。図 2 に、このプロセス全体を示します。この図では、サーバーのローカル・ファイルシステムが典型的な Linux ファイルシステム (ext4fs など) であることに注意してください。したがって、NFS は従来の意味でのファイルシステムではなく、リモートからファイルシステムにアクセスするためのプロトコルであると言えます。

図 2. NFS スタックのクライアントとサーバー
+--------------+                          +------------------------+
|    Client    |                          |      NFS Server        |
+------+-------+                          +-----+------------+-----+
       |                                        |            |
+------+-------+                          +-----+------------+-----+
|    VFS       |                          |     VFS                |
+------+-------+                          +-----+------------+-----+
       |                                        |            |
+------+-------+                          +-----+----+ +-----+-----+
|   NFS        |                          |   NFS    | | Local FS  |
+------+-------+                          +-----+----+ +----+------+
       |                                        |           |
+------+-------+                          +-----+----+ +----+------+
|  RPC/XDR     |                          |   RPC/XDR| | Local Disk|
+------+-------+                          +-----+----+ +-----------+
       |                                        |
+------+-------+                          +-----+----+
|   TCP/IP     |                          | TCP/IP   |
+------+-------+                          +-----+----+
       |                                        |
+------+-------+                          +-----+----+
|  Ethernet    |                          | Ethernet |
+------+-------+                          +-----+----+
       |                                        |
       |             +--------------+           |
       +-------------+  Network     +-----------+
                     +--------------+

高レイテンシーのネットワークの場合、NFSv4 はいわゆる複合プロシージャーを実装します。基本的に、複合プロシージャーはネットワークを介したリクエストの転送の負担を最小限に抑えるために、複数の RPC 呼び出しを 1 つのリクエストに統合します。さらに、レスポンス用のコールバック・スキームも実装します。


###############################################################################
NFS プロトコル

クライアントの観点では、NFS 内で行われる最初の操作はマウントと呼ばれます。マウントとは、リモート・ファイルシステムをローカル・ファイルシステム空間にマウントすることを意味します。このプロセスは、Linux システム・コールである mount の呼び出しとして始まり、VFS を介して NFS コンポーネントにルーティングされます。マウント用のポート番号が設定されると (これには、リモート・サーバーへの get_port リクエストによる RPC 呼び出しが使用されます)、クライアントは RPC による mount リクエストを行います。このリクエストは、クライアントと mount プロトコルを実行する特殊なデーモン (rpc.mountd) との間で行われます。このデーモンは、クライアントのリクエストを、サーバーに現在エクスポートされているファイルシステムのリストに照らし合わせます。リクエストされたファイルシステムがリストに存在し、クライアントがアクセスできる場合には、RPC による mount への応答によってファイルシステムに対するファイル・ハンドルが設定されます。クライアント・サイドは、リモート・マウント情報をローカル・マウント・ポイントと一緒に保管し、I/O リクエストを実行できるようにします。このプロトコルにはセキュリティー上の問題が発生する可能性があることから、NFSv4 ではこの補助的 mount プロトコルを内部 RPC 呼び出しに置き換えて、マウント・ポイントを管理しています。

ファイルを読み取るには、まずはそのファイルを開かなければなりません。RPC には OPEN プロシージャーが含まれていないため、クライアントはマウントされたファイルシステム内にディレクトリーとファイルがあるかどうかを単純にチェックするだけです。クライアントはまず、ディレクトリーに対する RPC リクエストとして GETATTR を発行します。このリクエストによって、ディレクトリーの属性が含まれるレスポンスが返されるか、ディレクトリーが存在しないことが示されます。次に、クライアントは RPC リクエスト LOOKUP を発行し、リクエストしたファイルが存在するかどうかを調べます。存在する場合には、そのファイルに対する RPC リクエスト GETATTR が発行され、それによってファイルの属性が返されます。GETATTR および LOOKUP が正常に完了した場合には、クライアントはファイル・ハンドルを作成します。ユーザーは、このファイル・ハンドルをその後のリクエストで使用することになります。

リモート・ファイルシステム内でファイルが特定されていれば、クライアントは RPC リクエスト READ を実行することができます。READ は読み取り操作のためのファイル・ハンドル、状態、オフセット、そしてカウントからなります。クライアントは状態を参照して操作を実行できるかどうか (つまり、ファイルがロックされているかどうか) を判別します。オフセットは読み取りを開始する位置を示し、カウントは読み取るバイト数を表します。サーバーはリクエストされたバイト数を返すことも、返さないこともありますが、RPC レスポンス READ の中には (データと併せて) サーバーが返すバイト数が示されます。

###############################################################################
NFS における技術革新

最新の 2 つのバージョンのNFS (バージョン 4 および 4.1) は、NFS のバージョンのうちで最も興味深く、NFS にとって最も重要なバージョンです。ここからは、NFS の進化の中でも特に重要な点をいくつか取り上げます。

NFSv4 より前のバージョンでは、マウントやロック、そしてファイル管理におけるその他の要素を対象とした補助的なプロトコルが多数ありました。NFSv4 はこのプロセスを 1 つのプロトコルにまとめることで簡素化し、トランスポート・プロトコルとしての UDP のサポートを取りやめました。NFSv4 はまた、UNIX および WindowsR ベースのファイル・アクセス動作のサポートを統合することによって、NFS を他のオペレーティング・システムにネイティブに統合できるように拡張します。

NFSv4.1 でスケーリング能力とパフォーマンスをより高めるために導入しているのは、pNFS (parallel NFS: パラレル NFS) の概念です。より高度なスケーリングをサポートするために、NFSv4.1 はクラスタリングされたファイルシステムと同じように、ストライピングによる分割データ/メタデータ・アーキテクチャーを実装します。図 3を見るとわかるように、pNFS はエコシステムをクライアント、サーバー、ストレージの 3 つに分割します。図に示されているとおり、このアーキテクチャーには 2 つのパスがあります。1 つはデータ用のパス、もう 1 つは制御用のパスです。pNFS はデータのレイアウトをデータ自体から切り離すことで、デュアルパス・アーキテクチャーを可能にします。クライアントがファイルにアクセスする必要があるときには、サーバーはレイアウトを返します。レイアウトが記述するのはファイルとストレージ・デバイスとのマッピングです。そのためレイアウトを受け取ったクライアントは、サーバーを介すことなく直接ストレージにアクセスできるようになります (したがって、スケーリング能力とパフォーマンスが高くなります)。クライアントはファイルを使用し終わると、データ (変更) とレイアウトをコミットします。必要な場合には、サーバーがクライアントにレイアウトを戻すようにリクエストすることもできます。

pNFS はこの振る舞いをサポートするためにいくつもの新しいプロトコル操作を実装しています。例えば、LayoutGet と LayoutReturn は、それぞれサーバーからレイアウトを取得、解放します。また、LayoutCommit はクライアントからストレージにデータをコミットして、他のユーザーがデータを使用できるようにします。サーバーがクライアントからレイアウトを回収するには、LayoutRecall を使用します。レイアウトは、並列アクセスおよびより高いパフォーマンスを実現するために複数のストレージ・デバイスにまで広がります。

図3.NFSv4.1 の pNFS アーキテクチャー
┌─────┐    ┌─────────────────────┐  
│NFS Client│    │                                          │
└─────┘    │                                          │
                  │              DataPath                    │    ┌─────┐
                ←│─────────────────────│─→│ Storage  │
                  │                                          │    └─────┘
┌─────┐    │                                          │
│NFS Client│    │                                          │
└─────┘    │                                          │
                  │                    Network               │    ┌─────┐
              ←←│──────────┐    ┌───────│─→│ Storage  │
                  │                    │    │              │    └─────┘
┌─────┐    │                    │    │              │
│NFS Client│    │                    │    │              │
└─────┘    │                    │    │              │
                  │                    │    │              │
                  │                    │    │              │
                  │                    │    │              │
                  └─────────────────────┘
                                        │    │ 
                                        ↓    ↓ 
                                     ┌─────┐
                                     │NFS v4.1  │
                                     │Server    │
                                     └─────┘

データとメタデータはどちらもストレージ域に保管されます。レイアウトを受け取ったクライアントは直接 I/O を実行できる一方、NFSv4.1 サーバーがメタデータの管理と保管を行います。この振る舞いは必ずしも新しいものではありませんが、pNFS はそこに、ストレージに対する複数のアクセス・メソッドをサポートする機能を追加しています。現在、pNFS はブロック・ベースのプロトコル (Fibre Channel)、オブジェクト・ベースのプロトコル、そして (pNFS の形でないとしても) NFS 自体の使用をサポートします。

NFS での作業は継続中であり、2010年 9月には NFSv2 の要件が公開されました。新たな機能強化の一部は、仮想環境で変わりつつあるストレージの世界に対応するためのものです。例えば、ハイパーバイザー環境ではデータが重複している可能性が大いに考えられます (多くのオペレーティング・システムが同じデータの読み取り/書き込み、キャッシングを行うため)。そのため、ストレージ・システム全体としてどこで重複が生じているのかを把握することが望まれます。それによって、クライアント・サイドではキャッシュ・スペースが保たれ、ストレージ・サイドでは容量が保たれることになります。そこで NFSv4.2 で提案しているのが、共有ブロックからなるブロック・マップによってこの問題に対処するという方法です。現在、ストレージ・システムは処理能力をバックエンドに統合し始めるようになっています。このことから、サーバー・サイドのコピーを導入し、ストレージのバックエンド自体で効率的にデータ・コピーを処理できる場合には、内部ストレージ・ネットワークからデータ・コピーの負担を取り除くという仕組みです。この他にも、フラッシュ・メモリーを対象としたサブファイルのキャッシングや、I/O に対するクライアント・サイドのヒント (おそらく mapadvise をパスとして使用) などの技術革新が登場し始めています。

###############################################################################
NFS に代わる手段

NFS は UNIX および Linux システムでは最もよく使用されているネットワーク・ファイルシステムですが、もちろん NFS だけが唯一の選択肢ではありません。WindowsR システムで最も広く採用されているのは、SMB (Server Message Block: CIFS とも呼ばれています) です。ただし、Linux が SMB をサポートするように、Windows も NFS をサポートします。

最新の分散ファイルシステムの 1 つで、Linux でもサポートされているのは Ceph です。Ceph は、POSIX (Portable Operating System Interface for UNIX) との互換性を備えた耐障害性分散ファイルシステムとして一から設計されました。Ceph について詳しく学ぶには、「参考文献」を参照してください。

その他の例には、Andrew 分散ファイルシステムの (カーネギー・メロン大学および IBM による) オープンソース・バージョンである OpenAFS、スケーラブル・ストレージとしての汎用分散ファイルシステムに重点を置く GlusterFS、そしてクラスター・コンピューティングを焦点とした超並列分散ファイルシステム Lustre などがあります。このすべてが、分散ストレージに対処するオープンソースのソフトウェア・ソリューションです。

###############################################################################
■NFSサーバ/クライアント環境構築
###############################################################################

┌────────┬───┬───┬───┬────┬────┐
│共有ディレクトリ│ Web  │ SMTP │ Pop  │ develop│ Client │
├────────┼───┼───┼───┼────┼────┤
│/export/www     │ rw   │      │      │ rw     │  r     │
├────────┼───┼───┼───┼────┼────┤
│/export/mail    │      │  rw  │ rw   │ rw     │  rw    │
├────────┼───┼───┼───┼────┼────┤
│/export/common  │      │      │      │ rw     │  rw    │
└────────┴───┴───┴───┴────┴────┘

                                       nfs
                                       nfs.domain.com
                                      ┌─────┐
                                      │ Server   │
                                      └─────┘
                                           │
───┬──────────┬──────────┬──────────┬──────────┬──
      │                    │                    │                    │                    │
┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐        ┌─────┐
│ Server   │        │ Server   │        │ Server   │        │ Server   │        │ Server   │
└─────┘        └─────┘        └─────┘        └─────┘        └─────┘
 web                   smtp                  pop                  develop                client
 web.domain.com        smtp.domain.com       pop.domain.com       develop.domain.com     client.domain.com
 192.168.1.100         192.168.1.101         192.168.1.102        192.168.1.103          192.168.1.104


上記の図のように、nfsサーバに「/export/www」「/export/mail」「/export/common」ディレクトリを共有してnfsクライアントにそのディレクトリへの権限を設定します。
 図内の表が、その共有ディレクトリへの対応表を示しており、ヘッダ部分はnfsクライアントのホスト名となっています。

■nfsサーバの共有ディレクトリ作成
共有ディレクトリを作成
# mkdir -p /export/www
# mkdir -p /export/mail
# mkdir -p /export/common

オーナ・グループ設定
# chown -R nfsnobody:nfsnobody /export
「オーナ・グループ」は後述するが、「nfsnobody」と言うデフォルトの匿名とさせる。
###############################################################################
■nfs設定ファイルの設定
nfs設定ファイルの設定
# vi /etc/exports
「/export/www」の設定
/export/www web(rw,all_squash,sync) develop(rw,all_squash,sync) client(ro,all_squash,sync)
「/export/mail」の設定
/export/mail smtp(rw,all_squash,sync) pop(rw,all_squash,sync) develop(rw,all_squash,sync) client(ro,all_squash,sync)
「/export/common」の設定
/export/common develop(rw,all_squash,sync) client(rw,all_squash,sync)


nfs設定ファイル「/etc/exports」の共有ディレクトリの設定は以下の書式となっています。
<directory> <client>(<option>) [<client>(<option>)] ・・・

directory：共有するディレクトリ
client   ：許可するホスト名またはFQDN(ドメイン名)またはネットワーク指定
           例) hogehoge               <-- ホスト名指定
               hogehoge.domain.com    <-- FQDN指定
               *.domain.com           <-- ワイルドカードによるドメイン名指定
               192.168.1.0/24         <-- ネットワーク指定
option   ：nfsクライアントに対するオプション

※ オプションの説明
┌───────┬────────────┐
│オプション    │ 説明                   │
├───────┼────────────┤
│ro            │ 読み込みのみ           │
├───────┼────────────┤
│rw            │ 読み書き               │
├───────┼────────────┤
│sync          │ 同期反映               │
├───────┼────────────┤
│async         │ 非同期反映             │
├───────┼────────────┤
│wdelay        │ 複数書込一括           │
├───────┼────────────┤
│no_wdelay     │ wdelayの逆             │
├───────┼────────────┤
│noaccess      │ 共有しない             │
└───────┴────────────┘
※ マッピングオプションの説明
┌───────┬─────────────────┐
│オプション    │ 説明                             │
├───────┼─────────────────┤
│all_squash    │ 全てのUID,GIDを匿名へ変換        │
├───────┼─────────────────┤
│anonuid       │ UIDを匿名アカウントに変換        │
├───────┼─────────────────┤
│anongid       │ GIDを匿名グループに変換          │
├───────┼─────────────────┤
│squash_uids   │ 指定UIDを匿名アカウントに変換    │
├───────┼─────────────────┤
│squash_gids   │ 指定GIDを匿名グループに変換      │
├───────┼─────────────────┤
│map_daemon    │ 動的なUID,GID変換を有効          │
├───────┼─────────────────┤
│map_static    │ UID,GIDの変換マップファイル指定  │
├───────┼─────────────────┤
│map_nis       │ NISベースのUID,GID変換を有効     │
├───────┼─────────────────┤
│root_squash   │ root特権をnfsnobodyに変換        │
├───────┼─────────────────┤
│no_root_squash│ root特権有効                     │
└───────┴─────────────────┘

備考1)「/export/common」を例にとりますが、以下の設定でも可能です

例1)
ホスト名とFQDNでの設定
/export/common develop(rw,all_squash,sync) client.domain.com(rw,all_squash,sync)
例2)
FQDNとIPアドレスでの設定
/export/common develop.domain.com(rw,all_squash,sync) 192.168.1.104(rw,all_squash,sync)
備考2)『nfsの設定』の図には準拠しませんが、192.168.1のセグメント全てに許可を与えることも可能です。

例)ネットワーク設定
/export/common 192.168.1.0/24(rw,all_squash,sync)
備考3)「rw (読書き可)」にも拘わらずクライアントから書込できない場合は、nfsサーバ側の「nfsnobody」UID:GIDを指定。

「nfsnobody」のUID確認
# cat /etc/passwd | grep nfsnobody
nfsnobody:x:4294967294:4294967294:Anonymous NFS User:/var/lib/nfs:/sbin/nologin
「nfsnobody」のGID確認
# cat /etc/group | grep nfsnobody
nfsnobody:x:4294967294:

例)
/export/common develop(rw,all_squash,sync,anonuid=4294967294,anongid=4294967294) client(rw,all_squash,sync,anonuid=4294967294,anongid=4294967294)

●NFS エクスポートの確認
NFS ClientからNFSエクスポートの確認をする場合はshowmountコマンドで一覧を見ることが可能。
# showmount -e <NFS Server hostname or IP Address>


###############################################################################
■ nfsの起動
● RHEL4 / RHEL5 の場合
※NFS は portmap を使用するので「portmap」を起動する必要があります。
portmapを起動する
# /etc/rc.d/init.d/portmap start

portmapを起動しなかった場合、以下のエラーメッセージが出力されます。
# /etc/rc.d/init.d/nfs start
NFS サービスを起動中:                                      [  OK  ]
NFS クォータを起動中: サービスを登録できません: RPC: 受け取れません; errno = 接続を拒否されました
rpc.rquotad: unable to register (RQUOTAPROG, RQUOTAVERS, udp).
                                                           [失敗]
NFS デーモンを起動中:                                      [失敗]
NFS mountd を起動中:  Cannot register service: RPC: Unable to receive; errno = Connection refused
                                                           [失敗]
RPC idmapd を起動中:                                       [  OK  ]

● RHEL6/RHEL7 の場合
※NFS は rpcbind を使用するので「rpcbind」を起動する必要があります。 

rpcbindを起動する
【RHEL6 の場合】
# /etc/rc.d/init.d/rpcbind start

【RHEL7 の場合】
# systemctl start rpcbind.service

rpcbindを起動しなかった場合、以下のエラーメッセージが出力されます。
# /etc/rc.d/init.d/nfs start
NFS サービスを起動中:                                      [  OK  ]
NFS クォータを起動中: サービスを登録できません: RPC: 受け取れません; errno = 接続を拒否されました
rpc.rquotad: unable to register (RQUOTAPROG, RQUOTAVERS, udp).
                                                           [失敗]
NFS デーモンを起動中:                                      [失敗]

###############################################################################
■NFS公開ディレクトリ(exportfs)
nfsサーバ起動後にnfs設定ファイル「/etc/exports」を変更した場合は以下のコマンドで可能です。
＊：nfsサービスを再起動する必要はありません

# exportfs -ra
＊：コマンド実行時に下記の様にメッセージ出力した場合は、クライアントの名前解決する必要があります。
hosts ファイルや DNS サーバを構築して、クライアント(web develop client smtp pop)を名前解決します。
 注：FQDNやショート名にご注意ください。

現在のエクスポートリストを表示
# exportfs -v

###############################################################################
■ nfsクライアントの設定
・ここでは、nfsクライアント(web.domain.com・smtp.domain.com・pop.domain.com・develop.domain.com・client.domain.com)での設定です。 
・nfsサーバ(nfs.domain.com)での設定ではないので注意してください。 

■ クライアント側の設定とマウント方法
マウント先を作成する
# mkdir /mnt/xxxxx
ディレクトリ名は任意です。
上記は説明上「/mnt/xxxxx」となっていますので、必要に応じてマウント先を作成して下さい。

以降のnfsクライアントのマウント先は、適当なディレクトリを作成済みとしています。

nfsサーバの共有ディレクトリをマウントする
＊：mountコマンドのパラメータ
mount <ファイルシステムタイプ> <nfsサーバ：共有名> <マウント先>

【web.domain.com の場合】
# mount -t nfs nfs.domain.com:/export/www /mnt/www

【smtp.domain.com および pop.domain.com の場合】
# mount -t nfs nfs.domain.com:/export/mail /mnt/mail

【develop.domain.com および client.domain.com の場合】
# mount -t nfs nfs.domain.com:/export/www /mnt/www
# mount -t nfs nfs.domain.com:/export/mail /mnt/mail
# mount -t nfs nfs.domain.com:/export/common /mnt/common

■ マウント時のエラーについて
以下の様にマウント時エラーが発生する場合
# mount -t nfs fedora.kajuhome.com:/export/xxxxx /mnt/xxxxx
mount.nfs: rpc.statd is not running but is required for remote locking.
mount.nfs: Either use '-o nolock' to keep locks local, or start statd.
mount.nfs: an incorrect mount option was specified

クライアント側でポートマッピングサービス「portmap」または「rpcbind」を起動する必要があります。
# /etc/rc.d/init.d/portmap start
または
# /etc/rc.d/init.d/rpcbind start

systemd services 系の場合のサービス起動は以下(例：rpcbind)
# systemctl start rpcbind.service

###############################################################################
■ nfsの動作確認
上記の動作確認を全て説明しても意味がないので、「rw権」「ro権」の両方が存在する「client.domain.com」に絞って説明します。

■ nfsクライアントでの動作確認

「ro」権の共有ディレクトリに試験ファイルを作成し保存してみる
# vi /mnt/www/client.dat
1234567890
abcdefghijklmnopqrstuvwxyz
「:wq」コマンドで保存
"/mnt/www/client.dat"
"/mnt/www/client.dat" E212: 書込み用にファイルを開けません
続けるにはENTERを押すかコマンドを入力してください

上記の様にメッセージ出力され、保存できない事が確認できる

「rw」権の共有ディレクトリに試験ファイルを作成し保存してみる
# vi /mnt/common/client.dat
1234567890
abcdefghijklmnopqrstuvwxyz
「:wq」コマンドで保存

作成できているか確認
# ls -l /mnt/common/client.dat
-rw-r--r--  1 nfsnobody nfsnobody 38  2月 16 16:04 /mnt/common/client.dat
 
■ nfsサーバでの動作確認
『nfsクライアントでの動作確認』で保存した「client.dat」を確認してみる
# cat /export/common/client.dat
1234567890
abcdefghijklmnopqrstuvwxyz
nfsクライアントで作成した内容を確認できた

＊：同様にnfsサーバで作成したファイルが、nfsクライアントで確認できるかも検証してみてください。
   「rw」権があるnfsクライアントは、作成されたファイルが確認できて保存も可能であること。
   「ro」権のnfsクライアントは、作成されたファイルが確認(読み取りのみ)できること。

■使用するポート一覧 (共通のもの)
┌───────┬──────────────────────────────┬─────┬─────┬─────┐
│実行ファイル名│ 役割                                                       │デフォルト│サーバに  │クライア  │
│              │                                                            │ポート    │必要      │ントに必要│
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│portmap       │ RPC系デーモンの使用ポートナンバーを管理する、いわばポート版│111 (TCP) │ ○       │ ○       │
│              │ のDNS のようなもの。NFS 関連だけでなく NIS (ypbind, ypserv)│111 (UDP) │          │          │
│              │ とruser,ruptimeなどの "r" 系サービスもその守備範囲         │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.nfsd      │ NFS サーバのユーザ空間プログラム。rpc.nfsd の役目はスレッド│2049 (UDP)│ ○       │ ×       │
│              │ の生成のみで、 NFS の根幹機能はカーネルモジュール nfsd.o が│2049 (TCP)│          │          │
│              │ 司る                                                       │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.mountd    │ マウント要求を受け付けるデーモン                           │動的      │ ○       │ ×       │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.rquotad   │ クォータの問い合わせに答えるデーモン。 クォータを使わない  │動的      │ △       │ ×       │
│              │ のであれば必要ない。RedHat 系なら /etc/sysconfig/nfs に    │          │          │          │
│              │ RQUOTAD=no と書いておけば、 rquotad は起動されなくなる。   │          │          │          │
│              │ ただし RedHat Enterprise Linux では副作用も                │          │          │          │
│              │ (「rquotadの使用/不使用でnfslockdのポートが変わる?」参照)  │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.lockd     │ 排他制御モジュール。NFS ロックマネージャ (NLM)。正確にはデ │動的      │ ○       │ ○       │
│              │ ーモンではなくカーネルモジュールであり、 2.2.18 より新しい │          │          │          │
│              │ カーネルの場合は敢えて起動する必要はなく、必要であれば自動 │          │          │          │
│              │ 的にロードされる                                           │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.statd     │ RPC プロトコルの ネットワークステータスモニタ (NSM) デーモ │動的      │ ○       │ ○       │
│              │ ン。マシンが不正終了した際に、それを lockd に伝え、ファイル│          │          │          │
│              │ ロック復帰の手助けをする                                   │          │          │          │
└───────┴──────────────────────────────┴─────┴─────┴─────┘

実行ファイル名/hosts_access名/RPCサービス名 対応表
┌───────┬───────┬─────┬───────┐
│実行ファイル名│ host_access名│ RPC名    │ OSサービス名 │
├───────┼───────┼─────┼───────┤
│portmap       │ portmap      │portmapper│rpcbind       │
├───────┼───────┼─────┼───────┤
│rpc.nfsd      │ nfsd         │nfs       │nfs           │
├───────┼───────┼─────┼───────┤
│rpc.mountd    │mountd        │mountd    │nfs           │
├───────┼───────┼─────┼───────┤
│rpc.rquotad   │rquotad       │rquotad   │nfs           │
├───────┼───────┼─────┼───────┤
│rpc.lockd     │lockd         │nlockmgr  │nfslock       │
├───────┼───────┼─────┼───────┤
│rpc.statd     │statd         │status    │nfslock       │
└───────┴───────┴─────┴───────┘

他のdaemonの使用するポートはシステムによって違います。この情報は
$ rpcinfo -p
で調べることができます

NFSを使用するためには、 portmapper, nfsd, statd, mountd, lockd 等を起動する必要があります。
このうち、portmapper(111)とnfsd(2049)はポートが固定されていますが、 statd, mountd, lockd,rquotadがどのポートを使用するかはportmapper頼みで、 使用ポートがどこに割り当てられるかわかりません。


■設定ファイル
/etc/sysconfig/nfs

MOUNTD_PORT=port
mountd (rpc.mountd) が使用する TCP と UDP ポートを制御します。 
コメント MOUNTD_PORT=892

STATD_PORT=port
(待機ポート)状態 (rpc.statd) が使用する TCP と UDP ポートを制御します。 
コメント STATD_PORT=662

STATD_OUTGOING_PORT=port
(送信ポート)状態 (rpc.statd) が使用する TCP と UDP ポートを制御します。 
コメント STATD_OUTGOING_PORT=2020

LOCKD_TCPPORT=port
nlockmgr (lockd) が使用する TCP ポートを制御します。 
コメント LOCKD_TCPPORT=32803

LOCKD_UDPPORT=port
nlockmgr (lockd) が使用する UDP ポートを制御します。 
コメント LOCKD_UDPPORT=32769

RQUOTAD_PORT=port
rquotadが使用する TCPとUDP ポートを制御します。
コメント RQUOTAD_PORT=875

RDMA_PORT=port
RDMAが使用する TCPとUDP ポートを制御します。
RDMA_PORT=20049


NFS が開始に失敗した場合、/var/log/messages をチェックします。通常は、すでに使用中のポート番号を指定した場合に NFS は開始に失敗します。/etc/sysconfig/nfs を編集した後に、service nfs restart を使用して NFS サービスを再開始します。rpcinfo -p コマンドを実行すると、その変化を確認できます。

●NFS を許可するためのファイアウォールの設定
1. NFS 用に TCP と UDP ポート 2049 を許可します。 
2. TCP と UDP ポート 111 (rpcbind) を許可します。 
3. TCP と MOUNTD_PORT="port" で指定された UDP ポートを許可します。 
4. TCP と STATD_PORT="port" で指定された UDP ポートを許可します。 
5. LOCKD_TCPPORT="port" で指定された TCP ポートを許可します。 
6. LOCKD_UDPPORT="port"で指定された UDP ポートを指定します。 
7. RQUOTAD_PORT="port"で指定された TCPとUDP ポートを許可します。

# iptables設定
$ vi /etc/sysconfig/iptables
# 下記を追記
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 2049 -j ACCEPT   #nfs[tcp]
  -A INPUT -m state --state NEW -m udp -p udp --dport 2049 -j ACCEPT   #nfs[udp]
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT    #rpcbind(portmapper)[tcp]
  -A INPUT -m state --state NEW -m udp -p udp --dport 111 -j ACCEPT    #rpcbind(portmapper)[udp]
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 892 -j ACCEPT    #mountd[tcp]
  -A INPUT -m state --state NEW -m udp -p udp --dport 892 -j ACCEPT    #mountd[udp]
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 662 -j ACCEPT    #statd[tcp]
  -A INPUT -m state --state NEW -m udp -p udp --dport 662 -j ACCEPT    #statd[udp]
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 32803 -j ACCEPT  #lockd[tcp]
  -A INPUT -m state --state NEW -m udp -p udp --dport 32769 -j ACCEPT  #lockd[udp]
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 875 -j ACCEPT  #rquotad[tcp]
  -A INPUT -m state --state NEW -m udp -p udp --dport 875 -j ACCEPT  #rquotad[udp]

###############################################################################
■ nfsの状態確認
アクセス状況の確認(アクセス種別ごとの回数、比率などが確認できます。)
# nfsstat -c
Client rpc stats:
calls      retrans    authrefrsh
1616       0          0

Client nfs v3:
null         getattr      setattr      lookup       access       readlink
0         0% 185      11% 13        0% 29        1% 129       8% 0         0%
read         write        create       mkdir        symlink      mknod
12        0% 35        2% 9         0% 6         0% 0         0% 0         0%
remove       rmdir        rename       link         readdir      readdirplus
1         0% 1         0% 2         0% 0         0% 0         0% 19        1%
fsstat       fsinfo       pathconf     commit
1162     72% 6         0% 0         0% 1         0%

実際のエクスポート状況を確認(NFS Server側)
# exportfs -v

NFS ClientからNFSエクスポートの確認をする場合はshowmountコマンドで一覧を見ることが可能。
# showmount -e <NFS Server hostname or IP Address>


###############################################################################
NFS通信
###############################################################################
■ファイルシステムがどのようにマウントされるか
クライアントがサーバーからファイルシステムをマウントするとき、そのファイルシステムに対応するファイルハンドルをサーバーから取得する必要があります。そのためには、クライアントとサーバーの間でいくつかのトランザクションが発生します。この例では、クライアントはサーバーから /home/terry をマウントします。snoop によって追跡したトランザクションは、次のとおりです。 

client -> server PORTMAP C GETPORT prog=100005 (MOUNT) vers=3 proto=UDP
server -> client PORTMAP R GETPORT port=33492
client -> server MOUNT3 C Null
server -> client MOUNT3 R Null 
client -> server MOUNT3 C Mount /export/home9/terry
server -> client MOUNT3 R Mount OK FH=9000 Auth=unix
client -> server PORTMAP C GETPORT prog=100003 (NFS) vers=3 proto=TCP
server -> client PORTMAP R GETPORT port=2049
client -> server NFS C NULL3
server -> client NFS R NULL3 
client -> server NFS C FSINFO3 FH=9000
server -> client NFS R FSINFO3 OK
client -> server NFS C GETATTR3 FH=9000
server -> client NFS R GETATTR3 OK

この追跡結果では、クライアントがまずマウントポート番号を NFS サーバーの portmap サービスに要求します。クライアントが取得したマウントポート番号 (33492) は、サーバーに対する存在確認のために使用されます。このポート番号でサービスが実行中であることが確認できると、クライアントはマウントを要求します。この要求により、サーバーはマウントされるファイルシステムに対するファイルハンドル (9000) を送ります。これに対してクライアントは、NFS ポート番号を要求します。クライアントはサーバーからポート番号を受け取り、NFS サービス (nfsd) を ping してから、ファイルハンドルを使ってファイルシステムに関する NFS 情報を要求します。

次の追跡結果では、クライアントは public オプションを使ってファイルシステムをマウントしています。 

client -> server NFS C LOOKUP3 FH=0000 /export/home9/terry
server -> client NFS R LOOKUP3 OK FH=9000
client -> server NFS C FSINFO3 FH=9000
server -> client NFS R FSINFO3 OK
client -> server NFS C GETATTR3 FH=9000
server -> client NFS R GETATTR3 OK

デフォルトの公共ファイルハンドル (0000) を使用しているために、すべてのトランザクションにポートマップサービスから情報が与えられ、NFS ポート番号を決定するためのトランザクションはありません。

#------------------------------------------------------------------------------
■ファイルロックの仕組み

    ┌────────────────┐          ┌─────────────────┐
    │ NFS Client Host                │          │ NFS Server Host                  │
    │                                │          │                                  │
    │                ┌─────┐  │          │  ┌─────┐                  │
    │                │ statd    │⇔│⇔⇔⇔⇔⇔│⇔│ statd    │                  │
    │ ┌─────┐ └─────┘  │ status   │  └─────┘                  │
    │ │ App      │      ↓         │          │       ↓                         │
    │ │ flock()  │ ┌─────┐  │          │  ┌─────┐                  │
    │ └─────┘ │ lockd    │→│→→→→→│→│ lockd    │                  │
    │system   ↓     └─────┘  │ nlockmgr │  └─────┘ →→┐     system│
    │call     ↓       ↑            │          │                     ↓       call│
    │────────────────│          │─────────────────│
    │ Kernel  ↓       ↑            │          │                     ↓     Kernel│
    │        ┌─────┐          │          │  ┌─────┐  ┌─────┐  │
    │        │ NFS      │→→→→→│→→→→→│→│ NFS      │⇒│ local    │  │
    │        │ Client   │          │ NFS      │  │ Server   │  │ fs       │  │
    │        └─────┘          │ Protocol │  └─────┘  └─────┘  │
    │                                │          │                                  │
    └────────────────┘          └─────────────────┘


###############################################################################
簡易手順
###############################################################################
上記内容を手順書化したもの。
#=== サーバ側の設定 ===#

# portmapperの動作確認
$ service portmap status

# nfsサービスで利用しているポート番号の確認
$ rpcinfo -p |grep nfs

# rpc.mountdのポート番号を固定
$ vi /etc/sysconfig/nfs
# 下記記述のコメントアウトを外す
MOUNTD_PORT=892

# iptables設定
$ vi /etc/sysconfig/iptables
# 下記を追記
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 2049 -j ACCEPT
  -A INPUT -m state --state NEW -m udp -p udp --dport 2049 -j ACCEPT
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 111 -j ACCEPT
  -A INPUT -m state --state NEW -m udp -p udp --dport 111 -j ACCEPT
  -A INPUT -m state --state NEW -m tcp -p tcp --dport 892 -j ACCEPT
  -A INPUT -m state --state NEW -m udp -p udp --dport 892 -j ACCEPT

# NFSサーバ起動の確認
$ service nfs status

# 起動していない場合
$ service nfs start

# nfsの自動起動
$ chkconfig nfs on

# 共有ボリュームの作成
$ mkdir -p /home/share
$ vi /etc/exports
# 共有ボリュームのパス, 接続許可ホスト, エクスポートプション (rw) = Read Write
/home/share 192.168.0.2(rw)

# 設定の反映
$ exportfs -a

# エクスポートテーブルの確認
$ exportfs -v

#=== 以下クライアント側の設定 ===#
$ mkdir -p /mnt/nfs
$ mount -t nfs 192.168.0.3:/home/share /mnt/nfs

# 共有ボリュームのオートマウント
$ vi /etc/fstab
# 下記記述を追記
192.168.0.3:/home/share /mnt/nfs nfs defaults 0 0

# クライアント側で下記エラーが出た場合、ポートマッピングサービス「portmap」または「rpcbind」を起動する必要があります
# mount.nfs: rpc.statd is not running but is required for remote locking.
# mount.nfs: Either use '-o nolock' to keep locks local, or start statd.
# mount.nfs: an incorrect mount option was specified
# /etc/rc.d/init.d/portmap start
または
# /etc/rc.d/init.d/rpcbind start

###############################################################################
一般的な NFS マウントオプション
###############################################################################
リモートホストに NFS を使用してファイルシステムをマウントする以外にも、マウントした共有を簡単に使用できるようマウント時に指定できるオプションがあります。これらのオプションは、mount コマンド、/etc/fstab 設定、autofs などを手作業で実行する場合に使用できます。 

以下に NFS マウントに一般的に使用されているオプションを示します。

●hard
エクスポートファイルシステムのホストが使用不能になった場合に、NFS 接続経由のファイルを使用するプログラムを停止して、サーバーがオンライン復帰するのを待つ。
hardを指定した場合は、intrオプションを一緒に指定しない限り、 NFS通信が再開するのを待つプロセスを終了することはできません。

●soft
エクスポートファイルシステムのホストが使用不能になった場合に、NFS 接続経由のファイルを使用するプログラムを停止して、エラーを報告する。 
softを指定した場合は、追加のtimeo=<value> オプションを設定できます。ここで、<value>はエラーが報告されるまでの経過秒数を指定します。

●intr
サーバーがダウンした場合やサーバーにアクセスできない場合に NFS 要求の割り込みを許可します。 
●lookupcache=mode
特定マウントポイントのディレクトリエントリーのキャッシュをカーネルにどのように管理させるかを指定します。mode に使用できる引数は、all、none、pos/positive になります。 

●nfsvers=version
使用する NFS プロトコルのバージョンを指定します。version は 2、3、4 のいずれかになります。複数の NFS サーバーを実行するホスト群に便利です。バージョン指定がない場合、NFS はカーネルおよび mount コマンドで対応している最近のバージョンを使用します。

vers オプションは nfsvers と同一であり、互換性を持たせる目的で本リリースに含まれています。 

●noacl
ACP の処理をすべてオフにします。旧式の Red Hat Enterprise Linux、Red Hat Linux、Solaris などの古いバージョンと連動させる場合に必要となることがあります。こうした旧式のシステムには最新の ACL テクノロジーとの互換性がないためです。 

●nolock
ファイルのロック機能を無効にします。この設定は旧式の NFS サーバーに接続する場合に必要となることが時折あります。 

●noexec
マウントしたファイルシステムでバイナリーが実行されないようにします。互換性のないバイナリーを含む Linux 以外のファイルシステムをマウントしている場合に便利です。 

●nosuid
set-user-identifier または set-group-identifier ビットを無効にします。リモートユーザーが setuid プログラムを実行しても必要以上の特権を得られないようにします。 

●port=num
port=num - NFS サーバーポートの数値を指定します。num が 0 (デフォルト) の場合、mount は、使用するポート番号についてリモートホストの rpcbind サービスのクエリーを実行します。リモートホストの NFS デーモンがその rpcbind サービスに登録されていない場合は、標準の NFS ポート番号 TCP 2049 が代わりに使用されます。 

●rsize=num and wsize=num
一度に転送するデータブロックサイズ (num はバイト単位) を大きめに設定することで NFS 通信の読み込み (rsize) と書き込み (wsize) の速度が上がります。旧式の Linux カーネルやネットワークカードの場合には、ブロックサイズが大きくなると正しく動作しなくなるものがあるため、これらの値を変更する際には注意してください。NFSv2 または NFSv3 の場合、読み込みと書き込みのいずれのパラメーターもデフォルト値は 8192 に設定されます。NFSv4 の場合、デフォルト値はいずれも 32768 に設定されます。 

●sec=mode
NFS 接続の認証時に利用するセキュリティータイプを指定します。デフォルトの設定は sec=sys になります。この設定は、ローカルの UNIX UID と GID を使用します。それらは NFS 操作の認証を行うために AUTH_SYS を使用します。 

sec=krb5 はユーザー認証にローカルの UNIX UID と GID ではなく Kerberos V5 を使用します。 
sec=krb5i はユーザー認証に Kerberos V5 を使用し、データの改ざんを防ぐため安全なチェックサムを使って NFS 動作の整合性チェックを行います。 

sec=krb5p はユーザー認証に Kerberos V5 を使用し、整合性チェックを実行し、トラフィックの傍受を防ぐため NFS トラフィックの暗号化を行います。これが最も安全な設定になりますが、パフォーマンスのオーバーヘッドも最も高くなります。 

●tcp
NFS マウントが TCP プロトコルを使用するよう指示します。 

●udp
NFS マウントが UDP プロトコルを使用するよう指示します。 

オプションの完全一覧および各オプションの詳細情報は、man mount および man nfs を参照してください。 

#####---------------------------------------------------------------------#####
hard,softメリットデメリット
ハードマウントの利点はNFSサーバがダウン等で応答しなくなった場合にもNFSクライアントは接続をキープすることです。

NFSサーバが正常に戻ると、NFSクライアントからは何ごともなかったかのように接続が再開されます。ただし、NFSダウン時に状態をキープすることで、なにも操作を受け付けなくなってしまいます（Ctrl+Cなどでも中断できなくなる）。これを回避するために、ハードマウント時にはintrオプションの使用が推奨されています。（interオプションにより中断が可能となります。）

ソフトマウントは接続失敗時にそのファイルアクセスを要求したプログラムに対してエラーを返します。ただし、このエラーに対応しているソフトウェアはあまり多くはなく、接続失敗によりデータが失われる可能性もあります。

以下URL先の情報によると、ソフトマウントはあまり推奨されていないようです。
http://www.linux.or.jp/JF/JFdocs/NFS-HOWTO/client.html

hardとsoftの違い
hard = レジューム機能あり
soft = レジューム機能なし
hardオプションはnfsマウントが切れてしまっても中断されるまで再思考を繰り返す。
softオプションはnfsマウントが切れてしまった場合、エラーを返し処理をしない。正しいデータではない可能性がある。
softの場合にはデータをアップロード中等のときにエラーを返すと、その場で処理を中断し、
アップロード側へはゴミファイルを残してしまう。再度アップロードしなおさなくてはならない。
このことから、確実に整合性の取れたデータをnfsさせたい場合にはhardを選択した方が良い(DBやメールなど)。

●hard の動作
・NFS サーバが応答するまで書込を永遠に繰返す。
・アプリケーションはI/Oを発行した後、完了待ちでスリープし続ける。
・hard と intr を併用するとシグナルを送ってI/Oを停止することができる。
kill -s SIGINT or SIGQUIT or SIGHUP <PID>

●soft の動作
・retrans で指定された回数書込に失敗すると、I/Oを発行したアプリケーションにエラーを返す。

どちらが良いか
整合性が求められるデータを読み書きに使う場合は hard にすべき。 ・不完全な書込*2や読込*3が発生する可能性があるため。

実行可能ファイルを置く場合も hard にすべき。
実行可能ファイルのデータをメモリに読込中やページアウトされたページを再読込中に、NFS サーバがクラッシュすると想定外の動作*4をする可能性がある。

#####---個人的見解---#####
よくあるシステムでsoftにしているところがあるがOS起動時にメイン業務では使用しない
(NFSマウントはバックアップ業務等)からNFSサーバがダウンした時のために直ぐに復旧出来るように
とsoftを選択している。
だが、計画的なOS起動時等は業務時間外なのでは？
NFSサーバの障害率なんて高いのでなければsoftによるオーバーヘッド(ファイルアクセスの度にstadとの通信を行う)を避けるためhardを選択すべきでは？

どう考えてもsoftとhardの関係は、
softによるオーバーヘッド > hardによるNFSサーバ障害率
さらにhard設定でintrオプションにすれば、中断出来るため推奨されていないsoftを選択する必要性がまったく感じられない。

#####---個人的見解---####
#####---------------------------------------------------------------------#####

###############################################################################
NFSでよくある問題点
###############################################################################
セキュリティ的に厄介な問題
次のようなケースの場合注意が必要です。

クライアント(myhost)
ログインユーザ名：me  (uid=500,gid=500)
ホームディレクトリのパス：/home/me
ホームディレクトリの中身： mydir/  myfile.txt  mypicture.gif

NFSサーバト(yourhost)
ログインユーザ名：you  (uid=501,gid=501)
ホームディレクトリのパス：/home/you
ホームディレクトリの中身： yourdir/   yourfile.txt yourpicture.gif

クライアントからマウントすると
#  mount -t nfs yourhost:/home/you /home/me/NFS 
#  cd /home/me/NFS 
#  ls -l 
drwxr-xr-x   2  he  he   中略    yourdir/ 
-rw-r--r--   1  he  he   中略    yourfile.txt  
-rw-r--r--   1  he  he   中略    yourpicture.gif

あれれ、自分のファイルとして扱われていないようですね。なんででしょう？ 
自分のホームディレクトリをマウントしたはずです。 
妙ですね・・・・・・・・ 
とこれは次のような理由なのです。 

ローカル（myhost）の /etc/passwd を見てみましょう。 
すると次のような記述があります。 
  
#  cat /etc/passwd 
（中略） 
： 
me:x:500:500:,,,:/home/me:/bin/tcsh 
he:x:501:501:,,,:/home/he:/bin/bash 
以下続く 
：
 
さて、ここでNFSサーバでの /home/you 以下のファイルの所有者は誰だったかを考えてみましょう。 
そうです、ユーザ名は you・userID は 501 ですね。 

つまりNFSでマウントした場合、リモートのUIDがそのまま受け継がれてくるのです。 
つまり 
  
#  mount -t nfs yourhost:/home/you /home/me/NFS 


によってマウントしたフォルダの所有者はマウントした時点で、 
ローカルホストのuserIDが501のユーザに引き継がれてしまうのです。 

/etc/passwd によるとローカルホストでUID=501 となるユーザは he というユーザですね。 
ですから上のようなことになるわけです。 
理由がわかれば簡単です。 

ですから、 NIS や LDAP などでユーザー情報を一元管理するか、クライアントとサーバのユーザーID を気をつけて設定する必要があります。

#------------------------------------------------------------------------------
NFSで困ったらtcpdumpでパケットを見るのもいいですが見ずらいため、rpcdebugで状態を見ることも可能です。
・サーバ側でデバッグログ出してみる
[root@nfs-server01 ~]# rpcdebug -m nfsd -s fh
nfsd       fh
[root@nfs-server01 ~]# tail -f /var/log/messages
Jul 31 16:09:35 nfs-server01 kernel: nfsd: fh_compose(exp fd:00/10049953 tmp/testdir, ino=0)
Jul 31 16:09:41 nfs-server01 kernel: nfsd: fh_compose(exp fd:00/10049953 testdir/testfile, ino=0)
Jul 31 17:01:27 nfs-server01 kernel: nfsd: fh_verify(20: 01000001 0000fd00 009959a1 009ded7d 26fbfccf 00000000)
[root@nfs-server01 ~]# rpcdebug -m nfsd -c fh
nfsd      <no flags set>

アクセスのあったファイル名が出力されています。
ただ読み込みについては初回以降はファイル名が出てこないのでよくわかりません。
 （最後の行。実は009ded7dがinode番号なのでfind -inumで見つけられなくはないです）

・クライアント側でデバッグログ出してみる
[root@nfs-client01 ~]# rpcdebug -m nfs -s vfs
nfs        vfs
[root@nfs-client01 ~]# tail -f /var/log/messages
Jul 31 17:14:02 nfs-client01 kernel: NFS: mkdir(0:19/10049953), testdir2
Jul 31 17:14:03 nfs-client01 kernel: NFS: dentry_delete(//testdir2, 0)
Jul 31 17:14:37 nfs-client01 kernel: NFS: create(0:19/10349951), testfile2
Jul 31 17:14:37 nfs-client01 kernel: NFS: dentry_delete(testdir2/testfile2, 0)
Jul 31 17:15:39 nfs-client01 kernel: NFS: dentry_delete(testdir2/testfile2, 8)
[root@nfs-client01 ~]# rpcdebug -m nfs -c vfs
nfs       <no flags set>
こちらのほうが見やすいですし、読み込みの際もファイル名が出てきてくれます。

###############################################################################
NFS性能チューニング
###############################################################################
■クライアントの転送ブロックサイズを変更
NFS を使い倒すつもりならば、ネットワーク設定の要件を満たすように rsize や wsize マウントオプションを調整してするのが必須になります。 

#------------------------------------------------------------------------------
server: /home/share  client_hostname(rw,sync,no_wdelay)
client: dev  -fstype=nfs,sync,rsize=4196,wsize=4196,hard  server_hostname:/home/share/dev

↓結果。
$ time dd if=/dev/zero of=./test.img bs=1024k count=5 
5+0 records in
5+0 records out
5242880 bytes (5.2 MB) copied, 2.45042 seconds, 2.1 MB/s

real    0m2.483s
user    0m0.000s
sys     0m0.000s
#------------------------------------------------------------------------------

#------------------------------------------------------------------------------
server: /home/share  client_hostname(rw,sync,no_wdelay)
client: dev  -fstype=nfs,sync,rsize=16384,wsize=16384,hard  server_hostname:/home/share/dev

↓結果。
$ time dd if=/dev/zero of=./test.img bs=1024k count=5
5+0 records in
5+0 records out
5242880 bytes (5.2 MB) copied, 2.13933 seconds, 2.5 MB/s

real    0m2.156s
user    0m0.000s
sys     0m0.000s
#------------------------------------------------------------------------------
###############################################################################
■NFSキャッシュの設定
NFSクライアントにキャッシュ機構を提供するcachefilesdというパッケージが提供されていますので、ディスクに余裕がある場合は使わない手はないと思います。まずyumからインストールを行います。

# yum install cachefilesd

設定ファイルは/etc/cachefilesd.confとなります。デフォルトで次のように設定されています。
# more /etc/cachefilesd.conf
###############################################################################
#
# Copyright (C) 2006,2010 Red Hat, Inc. All Rights Reserved.
# Written by David Howells (dhowells@redhat.com)
#
# This program is free software; you can redistribute it and/or
# modify it under the terms of the GNU General Public License
# as published by the Free Software Foundation; either version
# 2 of the License, or (at your option) any later version.
#
###############################################################################

dir /var/cache/fscache
tag mycache
brun 10%
bcull 7%
bstop 3%
frun 10%
fcull 7%
fstop 3%

# Assuming you're using SELinux with the default security policy included in
# this package
secctx system_u:system_r:cachefiles_kernel_t:s0

暗号のようなbrunやfrunの意味は次のようになり、bはファイルの容量、fはファイル数をそれぞれ管理することになります。
キャッシュの容量を制御オプション
┌─────┬─────────────────────────────────────┐
│パラメータ│ 詳細                                                                     │
├─────┼─────────────────────────────────────┤
│brun N%   │ 空き領域がディスクの合計容量の N% を上回ると cachefilesd は間引          │
│          │ きを無効にします。                                                       │
├─────┼─────────────────────────────────────┤
│bcull N%  │ 空き領域がディスクの合計容量の N% を下回ると cachefilesd は間引          │
│          │ きを開始します。                                                         │
├─────┼─────────────────────────────────────┤
│bstop N%  │ 空き領域が N% を下回ると cachefilesd は間引き動作によって空き領域        │
│          │ が N% を越えるまでディスク領域の割り当てを行わなくなります。             │
├─────┼─────────────────────────────────────┤
│frun N%   │ ファイルシステムがさらに格納できるファイル数が制限最大数の N% を         │
│          │ 上回ると cachefilesd は間引きを無効にします。                            │
├─────┼─────────────────────────────────────┤
│fcull N%  │ ファイルシステムがさらに格納できるファイル数が最大ファイル数の           │
│          │ N% を下回ると cachefilesd は間引きを開始します。                         │
├─────┼─────────────────────────────────────┤
│fstop N%  │ ファイルシステムがさらに格納できるファイル数が最大ファイル数の           │
│          │ N% を下回ると cachefilesd は、 間引きによって最大数の N% を越え          │
│          │ るようになるまでディスク領域の割り当てを行わなくなります。               │
└─────┴─────────────────────────────────────┘

とりあえずデフォルトの数値のままで良いのではとおもいます。これはデーモン形式で起動し、NFSをマウントする際に-o fscを指定することで有効にすることができますので/etc/fstabにそれを追記しておきます。

さしあたって気をつけるべきコマンドが、「dir」です。
キャッシュを保存するパスを、以下のように指定します。 
dir /var/cache/fscache

dir で指定するパスは、既存のファイルシステムでも構いませんし、 高速のストレージにパーティションを用意してもよいです

いずれにしましても、Ext4などのファイルシステムで、 「user_xattr」という拡張属性が必要になります。user_xattr が指定されるかどうかは、 tune2fs コマンドを実行すればわかります。たとえば、 /dev/sda5 の設定がどうか確認したければ、以下のように実行します。
# tune2fs -l /dev/sda5|grep user_xattr
  Default mount options:    user_xattr acl

こんな感じで、デフォルトのオプションに指定される設定になっているのであれば、 問題ありません。
そうでなければ、tune2fs コマンドで設定します。 
# tune2fs -o user_xattr /dev/sda5

すでにマウント中で、すぐ設定を反映したいなら、以下のようにします。 
# mount -o remount,user_xattr マウントポイント

というわけで、キャッシュするファイルシステムの用意ができましたら、 cachefilesd を実行しましょう。 
# service cachefilesd start

ちなみに、システム起動時に cachefilesd を自動起動させたい場合は、 以下を実行します。 
# chkconfig cachefilesd on   

さて、cachefilesd を起動できたら、対象のファイルシステムをマウントします。 いきなり FS-Cache の対象になるわけではなく、 「fsc」というオプションを指定してマウントする必要があります。

# mount -o fsc NFSサーバ:/マウント元 /マウント先

自動マウント時は下記のように記載。
# vi /etc/fstab
192.168.12.20:/new      /mnt/export     nfs   rsize=8192,wsize=8192,nosuid,hard,intr,fsc 0 0

そして、NFSマウントされた先を参照すると、
/var/cache/fscache 以下に、キャッシュされたファイルができます。
ただ、読み込んだファイルはキャッシュされますが、 書き込んだファイルはキャッシュされないようです。 

###############################################################################
■NFS スレッド数(/proc/fs/nfsd/threads)の変更
同時接続数の必要数を見積もって, nfsd のスレッド数を増加させる. (デフォルトは 8) /proc/fs/nfsd/threads に与えるか, rpc.nfsd -- 150 のようにオプションで与えるが, 通常は RPCNFSDCOUNT=150 のように起動スクリプト(/etc/init.d/nfsd)で指定する. 
数値を確認. 
# cat /proc/fs/nfsd/threads

nfsd の起動後でも変更できる. 
# echo 150 > /proc/fs/nfsd/threads

###############################################################################
NFSv4
###############################################################################
「良い意味でも悪い意味でも古いNFS」 を、インターネット経由でも使用できるようにと改良したのが、 NFSv4 すなわち NFS Version 4 だ。
NFS の Version 2 や 3 に比べると、mountd, lockd, statd 及び quotad の機能を NFS デーモン内部に取り込んだこと、(基本的には) portmapper が必要なく、サーバは 2049 番ポートだけ解放すればよくなったこと。
NFS over TCP が標準になったこと、デフォルトの読み取りブロックサイズが格段に大きくなったこと、ファイルのオープン/クローズやキャッシュなど多くのオペレーションをクライアントに任せるようになったことなどが特徴だ。

NFSv4 が NFSv3 よりもスピードで劣ることは考えにくい。特に、細かいファイルをたくさん開いたり閉じたりするオペレーションでは、(実装が正しければ) パフォーマンスは向上するはずだ。大きな単一ファイルでベンチマークを採っても差は出まい。

NFSv4 の動作に関係するデーモン
RedHat 系の RPM パッケージでは、portmap だけは portmap パッケージ、それ以外は nfs-utils パッケージに含まれる。

本来の NFSv4 の定義 (RFC 3530) からすると、portmap は要らないはずだ。しかし、RedHat EL 4, Fedora Core 3 及び 5 に実装されている nfsd では、portmap なしでは rpc.nfsd が起動できない。mountd もまた、要らないけれども要る。その辺りの実験報告は後半のコラム参照。

■使用するポート一覧 (共通のもの)
┌───────┬──────────────────────────────┬─────┬─────┬─────┐
│実行ファイル名│ 役割                                                       │デフォルト│サーバに  │クライア  │
│              │                                                            │ポート    │必要      │ントに必要│
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.idmapd    │ NFSv4 内部での制御に使用する NFSv4 ID と、UNIX ユーザ名と  │-         │ ○       │ ○       │
│              │ のマッピングを行うデーモン。RedHat系パッケージでは nfsd    │          │          │          │
│              │ 本体と同じ nfs-utils に含まれる。本来は nfsd が立ち上が    │          │          │          │
│              │ ってから idmapd を起動させるべきもののようだが、nfsd が    │          │          │          │
│              │ きちんと起動するには /etc/init.d/rpcidmapd の中に書かれ    │          │          │          │
│              │ ている sunrpc カーネルモジュールが要る、というおかしな     │          │          │          │
│              │ 依存関係になっている。そのため、/etc/init.d/rpcidmapd      │          │          │          │
│              │ は通常、マシン起動時に起動するよう /etc/rc3.d/ や rc5.d/   │          │          │          │
│              │ にリンクが作られているが、nfsd 起動スクリプトの中でも、    │          │          │          │
│              │ idmapd が立ち上がっていなければ `service rpcidmapd start'  │          │          │          │
│              │ するようなシェルが組まれている                             │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│portmap       │ RPC系デーモンの使用ポートナンバーを管理する、いわばポート版│111 (TCP) │ ○       │ ○       │
│              │ のDNS のようなもの。NFS 関連だけでなく NIS (ypbind, ypserv)│111 (UDP) │          │          │
│              │ とruser,ruptimeなどの "r" 系サービスもその守備範囲。       │          │          │          │
│              │ NFSv4 だけを使う場合は本来的には要らないが、RedHat系で     │          │          │          │
│              │ はしがらみ的に必要                                         │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.nfsd      │ NFS サーバのユーザ空間プログラム。rpc.nfsd の役目はスレッド│2049 (UDP)│ ○       │ ×       │
│              │ の生成のみで、 NFS の根幹機能はカーネルモジュール nfsd.o が│2049 (TCP)│          │          │
│              │ 司る                                                       │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.mountd    │ マウント要求を受け付けるデーモン                           │動的      │ ×       │ ×       │
│              │ NFSv4 の本来の仕様からすると不要なはずだが、RedHat系の実   │          │          │          │
│              │ 装方法では捨てられない。せめてもの抵抗として mountd に     │          │          │          │
│              │ `--no-nfs-versions 2',`--no-nfs-version 3' を与えて最小限  │          │          │          │
│              │ の状態で立ち上がらせる (パケットフィルタリングのための準備 │          │          │          │
│              │ の項を参照)                                                │          │          │          │
├───────┼──────────────────────────────┼─────┼─────┼─────┤
│rpc.rquotad   │ クォータの問い合わせに答えるデーモン。                     │動的      │ ×       │ ×       │
│              │ NFSv4 だけを使う場合は不要なので、RedHat 系では            │          │          │          │
│              │ /etc/sysconfig/nfs に RQUOTAD=no と書くことによって、      │          │          │          │
│              │ 起動しないようにしておく                                   │          │          │          │
└───────┴──────────────────────────────┴─────┴─────┴─────┘

実行ファイル名/hosts_access名/RPCサービス名 対応表
┌───────┬───────┬─────┐
│実行ファイル名│ host_access名│ RPC名    │
├───────┼───────┼─────┤
│rpc.idmapd    │ -            │-         │
├───────┼───────┼─────┤
│portmap       │ portmap      │portmapper│
├───────┼───────┼─────┤
│rpc.nfsd      │ nfsd         │nfs       │
├───────┼───────┼─────┤
│rpc.mountd    │mountd        │mountd    │
├───────┼───────┼─────┤
│rpc.rquotad   │rquotad       │rquotad   │
├───────┼───────┼─────┤
│rpc.lockd     │lockd         │nlockmgr  │
├───────┼───────┼─────┤
│rpc.statd     │statd         │status    │
└───────┴───────┴─────┘

■パッケージインストール後の調整
v4recoveryディレクトリの作成

RedHat系 nfs-utils RPMパッケージの不備だろう。サーバ側にあるべきディレクトリがひとつ足りないので、作っておく必要がある。RHEL4.5 でも Fedora Core 5 でもそうだった。

サーバ側ファイルシステムに /var/lib/nfs/v4recovery/ というディレクトリが存在するか確認して、なければ root 権限で、パーミッション 755 で作成する。v4recovery/ は、マウントが不意に切れたりロックに異常が起きた時のためのキャッシュを保存するディレクトリだと思われる。これがないと、nfsd を起動した時、システムログに "unable to find recovery directory /var/lib/nfs/v4recovery" というメッセージが出る。

■idmapd設定ファイル /etc/idmapd.conf
NFSv4 サーバとクライアントは、UNIX UID/GID とは別の NFSv4 ID というものによってユーザを区別している。それを UNIX ID に対応づけるのが rpc.idmapd の仕事だ。その設定ファイルである /etc/idmapd.conf は主に 3つのセクションから成る。

┌───────────────────────────────────────────────────────┐
│ [General] セクション                                                                                         │
├─────────┬─────────────────────────────────────────────┤
│ディレクティブ    │ 説明                                                                                     │
│                  │                                                                                          │
├─────────┼─────────────────────────────────────────────┤
│Verbosity         │ メッセージの冗長度                                                                       │
├─────────┼─────────────────────────────────────────────┤
│Pipefs-Directory  │ RPC Pipefs 仮想ファイルシステムの在処。nfs-utils を RedHat系 RPM でインストールした時の  │
│                  │ パスは /var/lib/nfs/rpc_pipefs。rpc_pipefs 仮想ファイルシステムのマウントは、sunrpc カ   │
│                  │ ーネルモジュールをロードすると行われ、その sunrpc モジュールは rpcidmapd 起動スクリプト  │
│                  │ の中でロードされるようになっている                                                       │
├─────────┼─────────────────────────────────────────────┤
│ Domain           │ NFSv4 はユーザを user@domain のカタチで扱うのでこれが必須。NFSv4 を Kerberos5 と組み合わ │
│                  │ せて使用しない限りは、必ずしも DNSドメインと同じである必要はないが、NFSサーバとクライア  │
│                  │ ントで同じ設定でなければならない。異なっていると、次項の Mapping セクションも実際のユー  │
│                  │ ザマッピングもきちんと機能せず、どのファイルもnobodyの所有であるかのように見えたりする。 │
├─────────┴─────────────────────────────────────────────┤
│ [Mapping] セクション                                                                                         │
├─────────┬─────────────────────────────────────────────┤
│ Nobody-User      │ root_squash, all_squash した際に割り付けられるデフォルトのユーザ。exports ファイルの説   │
│                  │ 明も参照                                                                                 │
├─────────┼─────────────────────────────────────────────┤
│ Nobody-Group     │ root_squash, all_squash した際に割り付けられるデフォルトのグループ。exports ファイルの   │
│                  │ 説明も参照                                                                               │
├─────────┴─────────────────────────────────────────────┤
│ [Translation] セクション                                                                                     │
├─────────┬─────────────────────────────────────────────┤
│ Method           │ UNIX側ユーザID/グループID の取得方法を指定する。指定可能な値には nsswitch と umich_ldap  │
│                  │ のふたつがあるが、後者はまだコードが未熟らしい                                           │
├─────────┴─────────────────────────────────────────────┤
│ [UMICH_LDAP] セクション (コードが未熟らしい)                                                                 │
├─────────┬─────────────────────────────────────────────┤
│ LDAP_SERVER など │ 前述の Method を umich_ldap にした場合のユーザ情報問い合わせ先 LDAP サーバや取得するアト │
│                  │ リビュート名などが指定できるらしい。CITI: Projects: NFS Version 4 Open Source Reference  │
│                  │ Implementation 参照。UMICH という変な名前は、NFSv4 実装の先駆者 CITI プロジェクトのある  │
│                  │ University of Michigan (ミシガン大学) に由来しているようだ。(RedHat EL4 でも rpc.idmapd  │
│                  │ と idmapd.conf の man に CITI のメンバーの氏名がクレジットされているのが見て取れる)      │
└─────────┴─────────────────────────────────────────────┘

現在のところ、設定可能な項目はやや不十分で、不足している部分は rpcidmapd 起動スクリプト冒頭の OPTIONS 変数に定義する。例えば、下記のようにすると、サーバ側の idmapd はサーバ専用、クライアント側はクライアント専用に働かせることができる；

サーバ側の init.d/rpcidmapd
OPTIONS="-S"

クライアント側の init.d/rpcidmapd
OPTIONS="-C" 

idmapd.conf に変更を加えたら必ず、サーバ、クライアントとも、
root# service rpcidmapd restart

■サーバ主設定ファイル /etc/exports
書式とパラメータ

書式：
directory client(option,option...) client(option,option...) ...

各パラメータの詳細：
directory:開放するディレクトリを指定

client:マウントを許可するクライアント。指定方法はいくつかある：

1.ホスト名か FQDN (例： somemachine, somemachine.hoge.cxm, あるいはワイルドカードを利用して *.hoge.cxm)
2.ネットグループ (例： @somegroup)
3.IP範囲 (例： 192.168.1.1-15 なら 192.168.1.0/255.255.255.240 または 192.168.1.0/28)。ワイルドカード併用不可

3. の指定方法を推奨する。その際には、「内部サブネットに属するアドレスは外部から入ってくるはずがない」 というポリシーをファイヤーウォールやルータに設定しておくと、安全性がさらに高まる。当家 iptalbes設定例 (特に bad_input チェーン) 参照。

option:
上記client部と、オプションの `(' との間にスペースを入れてはいけないことに注意。

┌─────────┬─────────────────────────────────────────────┐
│パラメータ        │ 説明                                                                                     │
├─────────┼─────────────────────────────────────────────┤
│fsid=num          │ 本来、fsid は、エクスポートするファイルシステムをマウントしているブロックデバイスのデバ  │
│                  │ イスナンバー (メジャー+マイナー) から計算されるらしいが、それを決め打ちしたい時に用いる。│
│                  │ 設定できる値は任意の 32bit値。これは NFSファイルサーバを冗長化している場合に、サーバが   │
│                  │ 切り替わった時にファイルハンドルが変に残ったりしないようにするために用いられるようだ。   │
│                  │ ただし、そうした用途でこのパラメータを設定することは希。NFSv4 特有の 0 という値を与えて  │
│                  │ 、エクスポートしたディレクトリを / (ルート) であるように見せるために用いるのが Linux で  │
│                  │ の実質唯一の用法(ここでは説明しきれない -- 下記設定例と、後述のクライアントの設定参照)   │
├─────────┼─────────────────────────────────────────────┤
│ro                │ 読み取り専用で共有。デフォルト。                                                         │
├─────────┼─────────────────────────────────────────────┤
│rw                │ 読み書き可能で共有。                                                                     │
├─────────┼─────────────────────────────────────────────┤
│sync              │ 延書き込みを有効にするか。遅延書き込みを有効 (async) にしておくと、サーバ上で実際に sync │
│async             │ が行われる前にサーバがリブートした場合データが壊れる恐れがある。NFS-1.1 以上では sync    │
│                  │ がデフォルト。                                                                           │
├─────────┼─────────────────────────────────────────────┤
│root_squash       │ root_squash を直訳すれば「root 権限つぶし」。クライアントから UID 0 としてのファイル操作 │
│no_root_squash    │ 要求を受けた際、権限を剥奪し、nobody なユーザにマッピングする。デフォルトは root_squash  │
│                  │ 有効。下記 anonuid, anongid によってマッピング先 UID/GID が指定できる。                  │
├─────────┼─────────────────────────────────────────────┤
│all_squash        │ 上記の拡大版で、 root に限らず全てのユーザが特定の 1ユーザにマッピングされる。デフォルト │
│no_all_squash     │ では no_all_squash つまり無効。マッピング先UID/GIDは下記 anonuid, anongid で指定できる。 │
├─────────┼─────────────────────────────────────────────┤
│anonuid=xxx       │ 上記の squash を行う際のマッピング先ユーザを指定したい場合に使用。idmapd.conf の         │
│anongid=xxx       │ Nobody-User, Nobody-Group の設定をこちらで上書きできる。ID は文字列呼称でなくナンバーで  │
│                  │ 指定しなければならない。つまり、マッピング先ユーザ/グループは、サーバとクライアントとで  │
│                  │ 同一の UIDナンバー/GIDナンバー、同一の文字列呼称を持っていないと、おかしなことになる。   │
├─────────┼─────────────────────────────────────────────┤
│no_subtree_check  │ subtree_check が有効 (デフォルト) だと、ファイルシステムの一部サブディレクトリだけがエク │
│subtree_check     │ スポートされている時、NFSリクエストが来る度に、操作対象のファイルが操作の許されたファイ  │
│                  │ ルシステム上にあるかどうかに加え、その親階層が確かにエクスポートされておりその上流のディ │
│                  │ レクトリが操作 (読み・書きなど) を許可しているかまでチェックする。セキュリティ的には     │
│                  │ subtree_check 有効に越したことはないが、多くの細かいファイルが存在するディレクトリをエク │
│                  │ スポートする際には、パフォーマンスを犠牲にしないために no_subtree_check を設定した方がよ │
│                  │ いとされている。                                                                         │
├─────────┼─────────────────────────────────────────────┤
│secure            │ secure (デフォルト) は、クライアントからの操作要求パケットの送信元ポートが 1024 より下、 │
│insecure          │ つまり特権ポートから発せられていないと受け付けない。insecure はこれをとやかく言わない。  │
├─────────┼─────────────────────────────────────────────┤
│hide              │ 例で説明しよう。サーバ上に、/var/export/ というディレクトリがあり、エクスポートしたいと  │
│nohide            │ する。ただし、その下位にある /var/export/hoge/ の内容は、じつは /home/chome/ をサーバ上  │
│                  │ でマウントしたものだったとする。この場合、hide (デフォルト) がセットされていると、NFSク  │
│                  │ ライアントで /var/export/ を NFSマウントしただけでは /home/chome/ にある内容は見えない。 │
│                  │ 見たければ、/var/export/ に加えてサーバが /var/export/hoge を明示的にエクスポートしてク  │
│                  │ ライアントがそれを /var/export/hoge/ に NFSマウントしなければならない                    │
└─────────┴─────────────────────────────────────────────┘
●単純な設定例
/home/hoge  192.168.1.0/28(ro,sync,fsid=0)

このようにエクスポーとすると、NFSv4 では、サーバ上の /home/hoge/ が server:/ として開放される。Windowsファイル共有のイメージに近い。
NFS 起動後に主設定ファイルを書き換えた場合は、 exportfs -ra コマンドを発行すれば変更が反映される。RedHat系 nfs INITスクリプトの場合は、`service nfs reload' でもほぼ同じことができる。意図通りにエクスポートされたかどうか確認するには、
root# exportfs -v

●複数のディレクトリを別々にエクスポートする例

現在のところ fsid=0 パラメータは /etc/exports の中で少なくとも 1回は唱える必要がある。これは NFSv4 が疑似ファイルシステム (pseudo-file system) という概念を採用しており、エクスポートされるオブジェクトは、概念上ひとつの基準ディレクトリの下位に属さなければならないからだ。そのため、少なくとも今の実装では、複数のディレクトリを別々の存在としてエクスポートするにはひと手間要る。例として、/var/export/ を server:/、 /home/hoge/data/ を server:/data/ として開放し、クライアントで server:/data/ 単独のマウントも可能にしたい場合を考えよう。

/etc/exports の記述：
/var/export       192.168.1.0/28(ro,sync,fsid=0)
/var/export/data  192.168.1.0/28(ro,sync,no_subtree_check,nohide)

まだ早合点して`exportfs -ra' してはいけない。 /home/hoge/data が出てこないじゃないか、と思っただろう。トリックはここから。サーバ上で、/home/hoge/data を /var/export/data/ に別名マウントしてやるのだ。別名マウントは mount コマンドの --bind 機能で行う。サーバ上で /home/hoge/data を /var/export/data としても参照できるようにしておくわけだ。やるべきことは、
root# mkdir -p /var/export/data
root# mount --bind /home/hoge/data /var/export/data

マシンを再起動しても bind が自動的に行われるようにするには /etc/fstab に下記のような 1行を加える；
/home/hoge/data  /var/export/data  none  bind  0 0

ファイルシステムのマウントは当然 nfsd などデーモンの起動よりも先に行われるので、めでたしめでたし。バインドマウントをやり直したい時には、一時的に共有を解除しないと "device is busy" と言われてアンマウントできないので、`exportfs -au' してから作業をする。なお、上記の例で言う /home/hoge/data の配下にもマウントによる枝が存在しそれも一緒にエクスポートしたい場合には、--bind の代わりに --rbind を使わなければならないかもしれない。

■クライアント側の設定
/etc/idmapd.conf の設定
/etc/idmapd.conf を設定する。サーバのものと同じ内容でなければならない。そして `service rpcidmapd restart'。

マウント 
単純に一時的にマウントするなら、例えば単純な例 の NFSサーバ hoge の /home/hoge を /mnt/export にマウントする場合、クライアント上で：
root# mount -t nfs4 hoge:/ /mnt/export -o hard,intr

とコマンドすれば良い。複数のディレクトリを別々にエクスポートする例 での /var/export/data だけをマウントする場合ならば、
root# mount -t nfs4 hoge:/data /mnt/export -o hard,intr

しばしばマウントするのなら、より最適なオプションを指定してクライアントの /etc/fstab に書いておくのが賢明。エントリは：
hoge:/  /mnt/export  nfs4  ro,nosuid,_netdev,noauto,hard,intr  0 0

主なマウントオプションの意味：
┌─────────┬─────────────────────────────────────────────┐
│パラメータ        │ 説明                                                                                     │
├─────────┼─────────────────────────────────────────────┤
│nosuid            │ 実行バイナリファイルに立っている SetUID ビットや SetGID を無視する。                     │
├─────────┼─────────────────────────────────────────────┤
│_netdev           │ このマウントエントリにはネットワークアクセスが必須であることを mount に伝え、ネットワー  │
│                  │ クが上がっていなければマウントを許さない。                                               │
├─────────┼─────────────────────────────────────────────┤
│noauto            │ mount -a による fstab エントリ一斉マウントに含めない (典型的にはブート時の自動マウント)  │
├─────────┼─────────────────────────────────────────────┤
│noexec            │ バイナリファイルの実行を禁止する。上の記述例では使用していないが、さらに厳重にするなら指 │
│                  │ 定しても良い。                                                                           │
├─────────┼─────────────────────────────────────────────┤
│rsize=xxx         │ データをサーバから読み取る際の転送ブロックサイズの最大制限値。 1024 の倍数にすべき。     │
│                  │ NFS Ver.4 でのデフォルトは 32768。ただしこれは上限であって、実際に遣り取りされる時のブロ │
│                  │ ックサイズは動的に変わるようだ。設定できる上限値はカーネルによって異なり、その値は カー  │
│                  │ ネルソースの include/linux/nfsd/const.h にある NFSSVC_MAXBLKSIZE で決まる。              │
├─────────┼─────────────────────────────────────────────┤
│wsize=xxx         │ データをサーバに書き込む際の転送ブロックサイズ。上記同様。                               │
├─────────┼─────────────────────────────────────────────┤
│hard              │ クライアント上のプログラムがファイルにアクセスしている最中にサーバがクラッシュした場合   │
│                  │ 、そのプログラムはタイムアウトせずに、サーバが回復するまで待ち続ける。相反するオプショ   │
│                  │ ンに soft があるが、そちらだとプログラムは自動的にタイムアウトするが、高い確率でファイル │
│                  │ が破損するらしい。                                                                       │
├─────────┼─────────────────────────────────────────────┤
│intr              │ interrupt の意 (たぶん)。上記 hard に伴ってハングしたプログラムを、中断や KILL できるよ  │
│                  │ うにする。                                                                               │
├─────────┼─────────────────────────────────────────────┤
│nfsvers=x         │ NFSv4 では指定無用。                                                                     │
├─────────┼─────────────────────────────────────────────┤
│mountvers=x       │ NFSv4 では mountd は使用しないので指定無用。                                             │
├─────────┼─────────────────────────────────────────────┤
│proto=tcp/udp     │ NFS サーバに対してどちらのプロトコルを使うかを指定。NFSv4 でのデフォルトは TCP。こうし   │
│                  │ て TCP だけに制限すると、ファイヤーウォールで開放しなければならないポートがより限定でき  │
│                  │ るというセキュリティ上のメリットがある。両ピア間がネットワーク的に非対称 (例えばサーバの │
│                  │ NICが100Mでクライアントが1000M) だったり、間にルータを挟んでいたりする場合は特に、TCP    │
│                  │ を使ったほうがトラブルを避けられるだろう。 NFS over UDP ではリトライが起きた場合にその   │
│                  │ 「ファイル」全体を再要求/再送してもらわなければならないが、 NFS over TCP ならば、落ちた  │
│                  │ パケットだけの再送で済むし、経路途中でのネットワークスピードの変化を上手に「吸収」してく │
│                  │ れるからだ。NFSv4 サーバの中には、TCP しか実装していないものもあるし、UDP で遣り取りした │
│                  │ いのなら NFSv3 を使い続けた方がいいだろう。                                              │
├─────────┼─────────────────────────────────────────────┤
│timeo=x           │ 要求(例えばファイル読取り要求) に対するサーバからの返答をどれだけの時間待つかの初期値。  │
│                  │ 単位は 1/10 秒。NFSv4 では、例えば初期タイムアウトが `100' つまり 10 秒であれば、最初、  │
│                  │ それだけ待ってサーバから返答が返ってこないと、次にクライアントは timeo を 2倍の 20 秒に  │
│                  │ して要求しなおし、それでもダメならさらに倍の 40 秒にしてまた再要求。これを、再送回数が   │
│                  │ retrans (次項参照) に達するか、timeo が 600 (60秒) に達するかのどちらか早いほうがやって  │
│                  │ くるまで繰り返す。NFS over TCP の初期 timeo のデフォルト値は 60 秒で、timeo の最大値は   │
│                  │ 60秒で固定されているので、その場合はもうそれ以上 timeo が延びることはない。 UDP の時の   │
│                  │ デフォルト値 `7' はややせっかち過ぎるきらいがあり、調整時の現実的な値は 50 (5秒) や      │
│                  │ 100(10秒)といった辺りのようだ。                                                          │
├─────────┼─────────────────────────────────────────────┤
│ retrans=x        │ 上記 timeo によって再要求が生じた場合に、何度まで再要求するか。TCP の場合のデフォルトで  │
│                  │ ある 2 回の場合、2回目の再要求でなおタイムアウトが起こると「メジャータイムアウト」となり │
│                  │ `server not responding' というメッセージを表示するとともに、あと 1回 (つまり 3回目) だけ │
│                  │ トライする。それでもダメな場合は、またマイナータイムアウトに立ち返り、 2回目の再要求で.  │
│                  │ ..となる。UDP の場合のデフォルト値は 5回。                                               │
├─────────┼─────────────────────────────────────────────┤
│sec=sys           │ ユーザ認証に使用する仕組みを指定。デフォルトは、システムの UNIX UID/GID を使う sys。筆者 │
│                  │ は Kerberos との併用は未検証だが、ローカル UID/GID の代わりに Kerberos V5 での認証を利用 │
│                  │ する krb5、認証に加えてチェックサムによるデータ改ざんチェックも行う krb5i、さらに上乗せ  │
│                  │ でトラフィックの暗号化も行う krb5p もある。                                              │
└─────────┴─────────────────────────────────────────────┘


●要らないはずなのに要る portmap と mountd 
少なくとも nfs-utils 1.0.8 で試した結果では、両方とも必須だった。まず、NFSサーバ側で portmap を立ち上げておかないと、nfsd は起動すら拒む。NFSv2 と v3 を無効にしておいても、どうやら、このバージョンの nfsd はローカル上の portmap に 1度はお伺いを立てずにいられないようだ。クライアント側もまた、 portmap を起動しておかないと "can't read superblock" というエラーが出て NFSv4 マウントに失敗する。 
mountd については、Fedora Core 5 で、 mountd 起動/停止記述を徹底的に排除した INITスクリプトを書いて試してみた。そうしても nfsd は正常に起動するし停止もできた。しかし、いざクライアントからマウントを掛けようとすると、  
mount: block device server:/ is write-protected, mounting read-only
 mount: cannot mount block device server:/ read-only  
と文句を吐いてやはりマウントが成り立たない。さらに言わせてもらえば、nfslock はスタートしていないのに nfsd を起動するだけで nlockmgr が rpcinfo の出力に現れるのも気になるし、クライアント側でパケットをキャプチャしてみるとサーバの mountd (?) からクライアントの RPC系動的ポートに向かって何やらパケットが送られるのも気になるところだ。

###############################################
NFSv4の詳細は下記を参照。
http://www.snia-j.org/tech/WH/NFSv4/NFSv41.html
###############################################

###############################################################################
#####   NFSv3 NFSv4の性能比   ############
###############################################################################
詳細は、下記URLを参照。
http://www.itmedia.co.jp/enterprise/articles/0807/18/news008.html

NFSv3からNFSv4に移行してもパフォーマンス面でのメリットはあまりないことが分かる。

実際のところ、NFSv4はファイル作成ではNFSv3の倍近い時間がかかり、ファイル削除ではNFSv3よりも高速である。圧倒的な速度向上のためにはasyncオプションが有効だが、これを使うとNFSサーバのクラッシュまたはリブート時に問題が生じるおそれがある。

■■■　NFSv4メリット　■■■
●ポート番号が2049 固定でファイアウォールと一緒に使いやすい
●Kerberos と組み合わせてセキュリティ面を強化
●idmapd で サーバー/クライアント間で異なるUID/GID でも対応できる

###############################################################################
NFSv4 + Kerberos で セキュリティとユーザーマッピングを解決
###############################################################################
NFSv4 と Kerberos を組み合わせることで、NFSのセキュリティとユーザーマッピングの課題を解決してみました。
これまで、自分が NFS を使う際の懸念事項がセキュリティの問題とユーザーマッピングの問題でした。

●セキュリティについて
/etc/exports でマウントを許可するクライアントを指定することはできるものの、IPアドレスは基本的に自己申告ですから、LAN内にある全てのマシンが信用できるという場合を除いては、かなり危険だと思います。
ある程度規模の大きい LAN につないでいる場合、中には悪い人がいるかもしれません。

例えば、サーバー側が /etc/exports に
/home    192.168.11.30(rw,sync)
と書いて、公開したとします。

他のマシンから
$ showmount -e NFS_SERVER_NAME

で問い合わせると
/home 192.168.11.30
と表示されます。

つまりどのホストに対して、どのディレクトリを公開しているかは一目瞭然です。 その IPアドレスになりすまされると、悪意のある人に簡単にマウントされてしまいます。

●ユーザーマッピングの問題について
NFSv3 までは、パーミッションについては クライアント側の UID と GID がサーバー側でも利用されるので、サーバー/クライアント間で UID と GID の同期が取れていない場合、別ユーザーになってしまうという問題があります。
例えば、サーバー側では taro が UID 500 だったとします。 もし、クライアント側で taro が UID 600 になっていたとすると、 taro は サーバー上のファイルへアクセスできなくなってしまいます。 しかも、クライアント側で、もし hanako が UID 500 に割り当たっていたとすると、 hanako は サーバー上の taro のファイルにアクセスできてしまうことになる。
ディストリビューションによって、一般ユーザーの UID に使われる範囲が違います。 RedHat系は 500～、Ubuntuなどでは 1000～ が使われるようです。
ですから、 NIS や LDAP などでユーザー情報を一元管理しているか、気をつけてユーザーID を設定している場合を除いては、サーバー/クライアント間で UID や GID は一致しないことが多いです。
そもそも「サーバー/クライアント間でユーザーに同じ UID/GID が使われている」という前提は期待されるべきものでもないと思うのです。
exports の古い man ページを見るとかつて map_static というオプションがあって、 サーバー/クライアント間で UID と GID を再マッピングできたみたいなのですが、すでに廃止されていているみたいです。

そこで、NFSv4 + Kerberos version5 をセットアップし、上記の「セキュリティの問題」と「ユーザーマッピングの問題」を解決してみました。
ユーザーマッピングについては NFSv4 で idmapd デーモンがやってくれるので、Kerberos までは必要ないようにも思えます。 ところが、NFSv4 単独だと、ユーザーマッピングがうまく動きませんでした。 検索してみたところ、 「idmapd が期待通り動かない」という書き込みが多かったですし、私も実際試してみたのですが、結局やり方がわかりませんでした。

一方、認証を Kerberos に任せた場合は、ユーザーマッピングもうまく動作しました。
RHEL/CentOS での設定をまとめます。
前提として、すでに Kerberos の基本的な設定は済んでいるものとします。 この部分は、前回 Kerberos を使ってみる(RHEL/CentOS編)の記事で説明しました。

●RHEL/CentOS編
RHEL6.3/CentOS6.3で確認しています。

ここでは説明上、
NFSサーバーのホスト名: nfsserver.mydomain.com
NFSクライアントのホスト名: nfsclient.mydomain.com
とする。

・rpc.gssd を有効化
NFSサーバー/NFSクライアント両方とも /etc/sysconfig/nfsで
SECURE_NFS="yes"
の行を有効にしておきます。
さらに、どのホストからでもいいですけど、
kadmin: addprinc -randkey nfs/nfsserver.mydomain.com
kadmin: addprinc -randkey nfs/nfsclient.mydomain.com

として、サーバー側、クライアント側の両方のサービスプリンシパルを作成する。
通常のサービスではサーバー側だけサービスプリンシパルを作成するのですが、NFS については サーバー/クライアント両方とも必要です。
ktadd はおのおののホスト上で行わないといけません。

NFSサーバー (nfsserver.mydomain.com)上で
kadmin: ktadd nfs/nfsserver.mydomain.com

NFSクライアント(nfsclient.mydomain.com)上で
kadmin: ktadd nfs/nfsclient.mydomain.com
とする。

それぞれのホストの /etc/krb5.keytab にキーが保存されるはず。
ちゃんと保存されているか念のために確認してみます。

NFSサーバー側の結果：
[root@nfsserver] # klist -e -k /etc/krb5.keytab
Keytab name: WRFILE:/etc/krb5.keytab
KVNO Principal
---- --------------------------------------------------------------------------
   2 nfs/nfsserver.mydomain.com@MYDOMAIN.COM (aes256-cts-hmac-sha1-96)
   2 nfs/nfsserver.mydomain.com@MYDOMAIN.COM (aes128-cts-hmac-sha1-96)
   2 nfs/nfsserver.mydomain.com@MYDOMAIN.COM (des3-cbc-sha1)
   2 nfs/nfsserver.mydomain.com@MYDOMAIN.COM (arcfour-hmac)
   2 nfs/nfsserver.mydomain.com@MYDOMAIN.COM (des-hmac-sha1)
   2 nfs/nfsserver.mydomain.com@MYDOMAIN.COM (des-cbc-md5)

NFSクライアント側の結果：
[root@nfsclient]# klist -e -k /etc/krb5.keytab
Keytab name: WRFILE:/etc/krb5.keytab
KVNO Principal
---- --------------------------------------------------------------------------
   3 nfs/nfsclient.mydomain.com@MYDOMAIN.COM (aes256-cts-hmac-sha1-96)
   3 nfs/nfsclient.mydomain.com@MYDOMAIN.COM (aes128-cts-hmac-sha1-96)
   3 nfs/nfsclient.mydomain.com@MYDOMAIN.COM (des3-cbc-sha1)
   3 nfs/nfsclient.mydomain.com@MYDOMAIN.COM (arcfour-hmac)
   3 nfs/nfsclient.mydomain.com@MYDOMAIN.COM (des-hmac-sha1)
   3 nfs/nfsclient.mydomain.com@MYDOMAIN.COM (des-cbc-md5)

また、NFSサーバー側の /etc/exports の書き方は
/home    gss/krb5(fsid=0,crossmnt,rw,sync)
などとなります。公開先のホスト名の部分を gss/krb5 とする部分がポイントです。

あとはクライアント側から
# mount -t nfs4 -o sec=krb5 nfsserver:/  hogehoge
といった感じでマウントができます。 (実は -t nfs4 の部分はなくてもよい。)

マウントはこれでできますが、実際にアクセスするには、ユーザーは Kerberos 認証を済ませないといけません。
$ kinit
でTGT を入手しておけば、アクセス可能になります。

なお、NFSv4 + Kerberos を解説したページで ktadd するときに -e des-cbc-crc:normal オプションを付けると説明しているページもありますが、現在はこのオプションは不要ですので、このオプションは付けないでください。 dec-cbc-crc 自体が廃止で、エラーになります。

###############################################################################
Kerberos を使ってみる (RHEL/CentOS編) 
###############################################################################
NFSv4 と Kerberos を組み合わせて、 NFS のセキュリティの弱さをカバーしてみたいと思います。
今回は Kerberos の初期設定の部分までやってみます。
まずは RHEL/CentOS の場合のやり方を説明ます。

●RHEL/CentOS編
RHEL 6.3/CentOS 6.3で確認しました。
Kerberos サーバー側は krb5-server パッケージをインストールします。 クライアント側については krb5-workstation がデフォルトでインストールされているので、特にインストールする必要はないです。
Kerberos サーバーのことを KDC (Key Distribution Center) と言います。

主に設定変更するファイルは3点です。
/etc/krb5.conf : Kerberos 認証を利用する全ホストで設定必要
/var/kerberos/krb5kdc/kdc.conf: KDCのみ設定する
/var/kerberos/krb5kdc/kadm5.acl: KDCのみ設定する

以下、説明の都合上、
KDCのホスト名 : mykdc.mydomain.com
サービスサーバーのホスト名: myserver.mydomain.com
サービスクライアントのホスト名: myclient.mydomain.com
として説明します。

mykdc と myserver は同一ホストであってもいいですが、一応、別々として説明しています。
Kerberos のシングルサインオンで、 myclient から myserver へ ssh ログインするところまでやってみます。

krb5.conf の修正
#------------------------------------------------------------------
各ホストの /etc/krb5.conf を以下のように修正します。
[logging]
 default = FILE:/var/log/krb5libs.log
 kdc = FILE:/var/log/krb5kdc.log
 admin_server = FILE:/var/log/kadmind.log

[libdefaults]
# default_realm = EXAMPLE.COM              ←コメントアウト
 default_realm = MYDOMAIN.COM              ←追加
 dns_lookup_realm = false
 dns_lookup_kdc = false
 ticket_lifetime = 24h
 renew_lifetime = 7d
 forwardable = true

[realms]
 EXAMPLE.COM = {
  kdc = kerberos.example.com
  admin_server = kerberos.example.com
 }

 MYDOMAIN.COM = {                            ←追加
  kdc = mykdc.mydomain.com                   ←追加
  admin_server = mykdc.mydomain.com          ←追加
 }                                           ←追加

[domain_realm]
 .example.com = EXAMPLE.COM
 example.com = EXAMPLE.COM
#------------------------------------------------------------------
レルム名 (Kerberos が管理する範囲のこと) が EXAMPLE.COM になっているので、 MYDOMAIN.COM に変更しました。 ドメイン名を大文字にしたものをレルム名にするのが慣例です。

kdc.conf の修正
#------------------------------------------------------------------
KDCホストの /var/kerberos/krb5kdc/kdc.conf を以下のように修正。
[kdcdefaults]
 kdc_ports = 88
 kdc_tcp_ports = 88

[realms]
# EXAMPLE.COM = {    ← コメントアウト
 MYDOMAIN.COM = {     ← 追加
  #master_key_type = aes256-cts
  acl_file = /var/kerberos/krb5kdc/kadm5.acl
  dict_file = /usr/share/dict/words
  admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab
  supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal
 }
#------------------------------------------------------------------

kadm5.acl の修正
#------------------------------------------------------------------
KDCホストの /var/kerberos/krb5kdc/kadm5.acl を以下のように修正。
# */admin@EXAMPLE.COM *   ← コメントアウト
*/admin@MYDOMAIN.COM *    ← 追加
#------------------------------------------------------------------
これは kadmin 中で実行可能なコマンドを設定しています。
*/admin のユーザーに全コマンドを許可するのが慣習です。

・データベース設定
3種類のファイルの修正ができたら KDC上で
root@mykdc # kdb5_util create -s
でデータベースを作成する。

途中で KDC database master key を入力するように言われるので、パスワードを設定します。
さらにサーバー側で kadmin.local で root/admin プリンシパルを作成する。 kadmin.local はどんなコマンドでもパーミッションを無視して、成功します。
#------------------------------------------------------------------
root@mykdc # kadmin.local
   ...
kadmin.local:  addprinc  root/admin
   ...
kadmin.local:  exit
#------------------------------------------------------------------

kadmin サービスと krb5kdc サービスを開始する。
#------------------------------------------------------------------
root@mykdc # chkconfig kadmin on
root@mykdc # service kadmin start
root@mykdc # chkconfig krb5kdc on
root@mykdc # service krb5kdc start
#------------------------------------------------------------------

ファイアウォールを設定している場合、
88/tcp, 88/udp (krb5kdc サービス)
749/tcp (kadminサービス)
を開けておきます。

これ以降は、どのホストからでも
$ kadmin -p root/admin
とすれば、KDC database のメンテができるようになります。

どのホストからでもいいのですが、以下のようにユーザープリンシパルやホストプリンシパルを作ります。
#------------------------------------------------------------------
$ kadmin -p root/admin
kadmin: addprinc user01
kadmin: addprinc -randkey host/myserver.mydomain.com
kadmin: exit
#------------------------------------------------------------------
とします。 user01 は使うユーザー名で適宜置き換えてください。

最後、ホストプリンシパルのパスワードを ktadd する。これは必ずサービスサーバー上で行わないといけない。
#------------------------------------------------------------------
root@myserver # kadmin -p root/admin
kadmin: ktadd host/myserver.mydomain.com
kadmin: exit
#------------------------------------------------------------------
キーはサービスサーバーの /etc/krb5keytab へ保存されます。

試しに、以下のようにするとキーが保存されているのがわかります。
#------------------------------------------------------------------
myserver # klist -e -k /etc/krb5.keytab 
Keytab name: WRFILE:/etc/krb5.keytab
KVNO Principal

   2 host/myserver.mydomain.com@MYDOMAIN.COM (aes256-cts-hmac-sha1-96) 
   2 host/myserver.mydomain.com@MYDOMAIN.COM (aes128-cts-hmac-sha1-96) 
   2 host/myserver.mydomain.com@MYDOMAIN.COM (des3-cbc-sha1) 
   2 host/myserver.mydomain.com@MYDOMAIN.COM (arcfour-hmac) 
   2 host/myserver.mydomain.com@MYDOMAIN.COM (des-hmac-sha1) 
   2 host/myserver.mydomain.com@MYDOMAIN.COM (des-cbc-md5) 
#------------------------------------------------------------------
以上でセットアップ完了です。

クライアント側で
user01@myclient $ kinit
で認証します。
#------------------------------------------------------------------
user01@myclient $ ssh myserver
#------------------------------------------------------------------
で myserver のログインパスワードを入力せずに、ssh 接続できます。 Kerberos で認証できているためです。シングルサインオンが実現しました！
#------------------------------------------------------------------
user01@myclient $ klist
でキャッシュされているチケットが表示されます。
Valid starting     Expires            Service principal
12/17/12 16:26:33  12/18/12 16:26:33  krbtgt/MYDOMAIN.COM@MYDOMAIN.COM
 renew until 12/17/12 16:26:33
12/17/12 16:27:17  12/18/12 16:26:33  host/myserver.mydomain.com@MYDOMAIN.COM
 renew until 12/17/12 16:26:33
#------------------------------------------------------------------

なお、 ホストプリンシパルのサービスチケットの取得がうまく行かない場合、 /etc/hosts を確認してみてください。
192.168.10.xx    myserver.mydomain.com myserver
のように Fully Qualified Domain Name が先にくるように書いておかないと、ホストの逆引きで失敗します。
192.168.10.xx    myserver myserver.mydomain.com
のように書くとうまくいきません。


###############################################################################
nfsstat - NFS の統計情報
###############################################################################
■コマンド
名前
nfsstat - NFS の統計情報を表示する 

書式
/usr/sbin/nfsstat [-anrcsz] [-o  facility ] ... 

説明
nfsstat は NFS クライアントとサーバの動作に関して保存されている統計を表示する。 

オプション
・-sサーバ側の統計のみを表示する。 デフォルトではサーバとクライアント両者の統計を表示する。
・-cクライアント側の統計のみを表示する。
・-n NFS の統計のみを表示する。 デフォルトでは NFS と RPC 両者の情報を表示する。
・-r RPC の統計のみを表示する。
・-o  facility 指定した facility の統計のみを表示する。以下のうちのひとつを指定できる。 
・nfs RPC コールを除く、NFS プロトコルの情報。
・rpc 一般的な RPC 情報 net ネットワーク層の統計。 例えば受信パケットの数、TCP 接続回数など。
・fh サーバのファイルハンドルキャッシュの利用情報。 ルックアップの回数、ヒットとミスの回数を含む。
・rc サーバのリクエスト返信用キャッシュの利用情報。 ルックアップの回数、ヒットとミスの回数を含む。 

ファイル
/proc/net/rpc/nfsd procfs ベースの kernel NFS サーバ統計へのインターフェース。 /proc/net/rpc/nfs procfs ベースの kernel NFS クライアント統計へのインターフェース。 

■クライアント側
マウント状況、オプションの確認
[root@nfs-client01 ~]# df -h
...
192.168.1.154:/tmp/     65G   47G   15G  77% /mnt

[root@nfs-client01 ~]# mount
...
192,168.1.154:/tmp/ on /mnt type nfs (rw,addr=192,168.1.154)
チューニングのためrsize、wsizeを大きくしていたり、
サーバ停止時の対策としてsoft、intr使ってるなら、ちゃんと入ってるかチェックしましょう

アクセス状況の確認
#------------------------------------------------------------------
[root@nfs-client01 ~]# nfsstat -c
Client rpc stats:
calls      retrans    authrefrsh
1616       0          0

Client nfs v3:
null         getattr      setattr      lookup       access       readlink
0         0% 185      11% 13        0% 29        1% 129       8% 0         0%
read         write        create       mkdir        symlink      mknod
12        0% 35        2% 9         0% 6         0% 0         0% 0         0%
remove       rmdir        rename       link         readdir      readdirplus
1         0% 1         0% 2         0% 0         0% 0         0% 19        1%
fsstat       fsinfo       pathconf     commit
1162     72% 6         0% 0         0% 1         0%

Client nfs v4:
null         read         write        commit       open         open_conf
0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
open_noat    open_dgrd    close        setattr      fsinfo       renew
0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
setclntid    confirm      lock         lockt        locku        access
0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
getattr      lookup       lookup_root  remove       rename       link
0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
symlink      create       pathconf     statfs       readlink     readdir
0         0% 0         0% 0         0% 0         0% 0         0% 0         0%
server_caps  delegreturn
0         0% 0         0%
#------------------------------------------------------------------
アクセス種別ごとの回数、比率などが確認できます。

■サーバ側
export状況の確認

設定ファイルを確認
#------------------------------------------------------------------
[root@nfs-server01 ~]# cat /etc/exports
/tmp 192.168.1.0/24(rw,no_root_squash)
#------------------------------------------------------------------
実際のエクスポート状況を確認
#------------------------------------------------------------------
[root@nfs-server01 ~]# exportfs
/tmp            192.168.1.0/24

[root@nfs-server01 ~]# showmount -e
Export list for nfs-server01:
/tmp   192.168.1.0/24
#------------------------------------------------------------------
設定と差分があるようならexportfs -rか/etc/init.d/nfs reloadで反映させましょう

■被マウント状況の確認

showmountで確認できます
#------------------------------------------------------------------
[root@nfs-server01 /]# showmount -a
All mount points on nfs-server01:
192.168.1.153:/tmp
#------------------------------------------------------------------
正常にumountされなかったときの情報が残っていることがあるので、
 正確な情報を得るためには出力された各クライアントにログインして調べる必要があると思います。
 （もっといい方法ないのかな・・・）

■アクセス状況の確認
#------------------------------------------------------------------
[root@nfs-server01 ~]# nfsstat -s
Server rpc stats:
calls      badcalls   badauth    badclnt    xdrcall
2640       0          0          0          0

Server nfs v3:
null         getattr      setattr      lookup       access       readlink
11        0% 162       6% 9         0% 21        0% 122       4% 0         0%
read         write        create       mkdir        symlink      mknod
12        0% 23        0% 7         0% 6         0% 0         0% 0         0%
remove       rmdir        rename       link         readdir      readdirplus
1         0% 1         0% 0         0% 0         0% 0         0% 12        0%
fsstat       fsinfo       pathconf     commit
2228     84% 8         0% 0         0% 0         0%
#------------------------------------------------------------------
アクセス種別ごとの回数、比率などが確認できます。

■rpcdebugというコマンドを紹介しておきます。
このコマンドでフラグを指定するとデバッグログが出力されるようになります。
・フラグ一覧の確認
#------------------------------------------------------------------
[root@nfs-server01 ~]# rpcdebug -vh
usage: rpcdebug [-v] [-h] [-m module] [-s flags...|-c flags...]
       set or cancel debug flags.

Module     Valid flags
rpc        xprt call debug nfs auth pmap sched trans svcsock svcdsp misc cache all
nfs        vfs dircache lookupcache pagecache proc xdr file root callback all
nfsd       sock fh export svc proc fileop auth repcache xdr lockd all
nlm        svc client clntlock svclock monitor clntsubs svcsubs hostcache xdr all
#------------------------------------------------------------------

・サーバ側でデバッグログ出してみる
#------------------------------------------------------------------
[root@nfs-server01 ~]# rpcdebug -m nfsd -s fh
nfsd       fh


[root@nfs-server01 ~]# tail -f /var/log/messages
...
Jul 31 16:09:35 nfs-server01 kernel: nfsd: fh_compose(exp fd:00/10049953 tmp/testdir, ino=0)
...
Jul 31 16:09:41 nfs-server01 kernel: nfsd: fh_compose(exp fd:00/10049953 testdir/testfile, ino=0)
...
Jul 31 17:01:27 nfs-server01 kernel: nfsd: fh_verify(20: 01000001 0000fd00 009959a1 009ded7d 26fbfccf 00000000)
...

[root@nfs-server01 ~]# rpcdebug -m nfsd -c fh
nfsd      <no flags set>
#------------------------------------------------------------------
アクセスのあったファイル名が出力されています。
ただ読み込みについては初回以降はファイル名が出てこないのでよくわかりません。
 （最後の行。実は009ded7dがinode番号なのでfind -inumで見つけられなくはないです）


・クライアント側でデバッグログ出してみる
#------------------------------------------------------------------
[root@nfs-client01 ~]# rpcdebug -m nfs -s vfs
nfs        vfs

[root@nfs-client01 ~]# tail -f /var/log/messages
...
Jul 31 17:14:02 nfs-client01 kernel: NFS: mkdir(0:19/10049953), testdir2
...
Jul 31 17:14:03 nfs-client01 kernel: NFS: dentry_delete(//testdir2, 0)
...
Jul 31 17:14:37 nfs-client01 kernel: NFS: create(0:19/10349951), testfile2
...
Jul 31 17:14:37 nfs-client01 kernel: NFS: dentry_delete(testdir2/testfile2, 0)
...
Jul 31 17:15:39 nfs-client01 kernel: NFS: dentry_delete(testdir2/testfile2, 8)

[root@nfs-client01 ~]# rpcdebug -m nfs -c vfs
nfs       <no flags set>
#------------------------------------------------------------------
こちらのほうが見やすいですし、読み込みの際もファイル名が出てきてくれます。



###############################################################################################
様々なNFSサーバがマウントされているサーバで、ある特定のNFSサーバを全てアンマウントしたいとき

# df -hP -t nfs | grep "^対象サーバの条件(IPアドレスとか)" | awk {'print $6;'} | xargs umount

ちなみにmountコマンドだと、次の通り
# mount -t nfs | grep "^対象サーバの条件(IPアドレスとか)" | awk {'print $3;'} | xargs umount

もっとスマートやり方だと
# umount -a -t nfs -O addr=IPアドレス



