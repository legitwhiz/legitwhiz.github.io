VMware のパフォーマンス･テストでは、このオーバーヘッドが 8% ～ 12% になると示唆している。

インテル バーチャライゼーション・テクノロジー　Intel Virtualization Technology

仮想化のオーバーヘッドを低減し、複数OS の並行動作をより効率的に行うために、インテル社がハードウェアとして、プロセッサに組み込んだ仮想化支援機構。Intel VTは、「Intel VT-x」、「Intel VT-i」、「Intel VT-d」などの仮想化支援機能の総称。

VT-x
VT-xは、Core 2 Duoプロセッサ, Xeonプロセッサなど向けに実装された仮想化支援技術。ハイパーバイザとの連携により、複数OSが個別に実行する制御命令を矛盾無く並列実行させる機能を備えている。

VT-i
VT-iは、Itaniumプロセッサ向けに実装された仮想化支援技術で、VT-xと同様な機能を有する。

VT-d
VT-dは、ディスクアクセスやネットワーク通信といったI/O処理のための仮想化支援技術。

【仮想化によるオーバヘッド】
完全仮想環境でホスト型仮想化ソフトを利用した場合の、性能上オーバヘッドになる部分をまとめてみた。一応、自分なりに4つにまとめてみた。まあ、これ以外にも細かい点ではいっぱいあるだろうがｗ
赤字は、その問題を解決する下記で紹介されている技術。ちなみに、この中で今一番の仮想化の問題は
③IOデバイス（ディスク、ネットワーク、グラフィック）のドライバ二重介在による処理変換のオーバヘッド
だろうなー。ハイパーバイザ型の仮想化ソフトでもこの問題は顕在化しているし、仮想化がIOボトルネックになりやすいと言われる所以でもある。


①CPU特権命令変換によるオーバヘッド
　x86／x64アーキテクチャではring0、1、2、3の４つの動作モードがあり、それぞれの動作モードによってプログラムがアクセスできるメモリ空間を分けている。ring0は主にOSカーネルの特権命令に、ring3はOS上のアプリケーションの命令の際に使用される（ring1、2は現在は使用されていない）。
　しかし、仮想マシンOS上からのring0命令はホストOSから見るとring3命令であり、この命令はエラーになってしまう。ホスト型仮想化ソフトのVMMでは、このエラーを常に監視しており、エラーが出た際は特権命令をVMM上で代理処理（エミュレーション）を行いゲストOSに処理結果を返している。この監視とエミュレーションによってオーバヘッドが生じる。
⇒仮想化支援機能 Intel VT-x、ハイパーバイザ

②CPUメモリアクセスのアドレス変換によるオーバヘッド
　仮想マシンが使用しているメモリアドレスと物理マシンが使用しているメモリアドレスは当然異なる。そのため、VMMは仮想マシンから見えている物理メモリと実際の物理メモリそれぞれのアドレスを変換処理（マッピング）しなくてはならない（実際には仮想マシンの仮想メモリ→仮想マシンの物理メモリ→物理マシンの仮想メモリ→物理マシンの物理メモリという4段階のマッピングｗ）。このメモリのマッピングにオーバヘッドが発生している。
⇒仮想化支援機能 Intel EPT

③IOデバイス（ディスク、ネットワーク、グラフィック）のドライバ二重介在による処理変換のオーバヘッド
　最近の物理マシンのIOデバイス処理は、DMA(Direct Memory Access：CPUを介さずにデバイスとRAMの間で直接データ転送を行なう方式)によって処理が行われている。しかし、仮想マシンからは本物のデバイスも物理メモリのアドレスも見えない。そのため、仮想マシンからIO処理を行うには、まず仮想マシン上の擬似的なデバイスドライバで処理が受け付けられ、そこから更にホストOS（もしくはVMM）上の実際のデバイスドライバへ処理が変換され、実際のデバイスドライバがIO処理を行うようにコントロールされている。その際の処理変換にオーバヘッドが生じてしまい、IOデバイス性能が仮想マシンを利用した場合落ちてしまう。IO処理の例としては、ディスクIO、ネットワークIO、グラフィック表示等がある。
⇒仮想化支援機能 Intel VT-d、準仮想化

④複数の仮想OSの状態情報切り替え（コンテキストスイッチ）によるオーバヘッド
　複数の仮想マシンが起動していた場合、各OSごとの処理の切り替えをする際、CPUレジスタの状態（コンテキスト）を保存・復元する処理が必要になってくる。この状態情報を切り替える作業（コンテキストスイッチ）も仮想化のオーバヘッドの一つである。　
⇒仮想化支援機能 Intel VT-x、ハイパーバイザ



【仮想化の性能を向上させる3つの技術】
□仮想化支援機能
　仮想化支援機能は、CPUやチップセット等のハードウェアレベルで仮想化技術を支援する技術である。IntelとAMDの技術の2種類があるが、どちらも大まかな働きの違いはないので、下記ではIntelベースで説明。

①Intel VT-x（x86系）、Intel VT-i（Itanium系）、AMD-V
　Intel VT-xでは、「VMX Rootモード」と「VMX Non-rootモード」という、新たなプロセッサの動作モードが2種類用意された。仮想マシンでの通常処理はVMX Non-rootモード（のring3とring0）で動作し、仮想マシンから特権命令が実行され際、VMX RootモードになりVMMに処理が移る。これによって、VMMは仮想マシンの特権命令の監視とエミュレーションをいちいちしなくて済むため、オーバヘッドがある程度軽減される。
　また、Intel VT-xでは仮想マシンのレジスタ内容をVMCS（Virtual Machine Control Structure）と呼ばれる専用テーブルに格納する。VMX non-rootモードで動作していたCPUの状態の保存や読み出しに利用される。これによって、コンテキストスイッチによるオーバヘッドを軽減している。

出展
・Intelの仮想化支援機能「Intel VT」とは？
http://www.atmarkit.co.jp/fsys/kaisetsu/085intelvt/intelvt.html
・仮想化技術の性能を向上させる、ハードウェア仮想化支援機能とは？
http://www.atmarkit.co.jp/fwin2k/tutor/intelvtx/intelvtx_02.html

※ただし、Intel-VTのようなCPUの仮想化支援機能が必ずしも性能向上につながるわけではない。
・VMWare Communities：仮想マシンのモニターモード(VMMの実行モード)を確認する
http://communities.vmware.com/blogs/kkomatsu/2009/07/26/-vmm-
では、下記のように述べられている。


物理CPUによる仮想化支援のうち、CPUコマンドの処理を支援する機能はIntelではVT-xと呼んでいる。ESXがこれまでVT-xをほとんど利用してこなかった理由はいくつかあるが、主だった理由は下記の2つだ。

・特権命令やセンシティブな命令を安全に処理するVMX Rootモードと、一般的なCPU命令を処理するVMX non-rootモードを切り替えるVMEnter/VMExitの際の遅延が大きい (VMCALL/VMRESUMEコマンドの遅延が大きい) 
・モードの切り替えの際に、TLB（Translation Look-aside Buffer：CPUが仮想アドレスと論理アドレスとを対応させた情報を一時的に保管しておくバッファメモリ ）を完全にフラッシュされてしまう 

実際、VMWare ESX 3.5ではIntel VTに対応していないCPUでも使用することができたし（他のハイパーバイザ型仮想化ソフトではIntel VT必須）、VMWare Workstation 6.5でもIntel VTを有効にしても性能はほとんど変わらない。

ジオングに足をつけた程度ということだろう。

②Intel EPT(Extended Page Tables)（Intel VT-xの一部）、AMD NPT(Nested Page Tables)
ページテーブルっていうのは、仮想アドレスと物理アドレスのマッピング情報を格納するテーブルのことで、ページング方式のメモリ管理をしているOSは皆利用している。さらにそれを仮想マシンと物理マシン間でも同じようなの作りましたっていうのがIntel EPT。メモリアドレス変換機構をCPU内に搭載している。

ITproから引っ張ってきたIntel EPTのわかりやすい図↓
 

③Intel VT-d、AMD IOMMU
　Intel VT-dは、IOデバイスが仮想マシン上からDMAを行うために、仮想マシンと実際の物理メモリのアドレスを自動的に変換する機構(リマッピングエンジン)をnorth-bridgeのチップセットに組み込むことによって（CPU側、チップセット側両方のサポートが必要）、仮想マシンが直接DMAを制御することが可能になる技術である。これによって、IOデバイスを仮想マシン上から直接制御することが可能になり、ディスク、ネットワーク、グラフィック処理の性能向上が期待できる。
※ただし、現在Intel VT-dでサポートされているIOデバイスは、PCI Express接続の機器だけらしい。

　また、IOデバイスを仮想マシンから直接扱うことによって問題もある。それは、仮想マシンでひとつのIOデバイスを占有してしまうため、従来のように複数の仮想マシンで共有することができないことだ。これについての対策として、IOデバイス自体の仮想化のための規格「PCI-SIG I/O Virtualization (IOV)」の開発がPCI-SIGで現在進められている。

　まあ、最近（2009年）市場に出たばかりの技術なので、まだまだ未知の部分は多い・・・

出展
・IDF 2008で見たIntelの仮想化対応策 
http://enterprise.watch.impress.co.jp/cda/topic/2008/09/05/13785.html
・第2回　チップ・セットに支援機能を搭載してI/Oを仮想化
http://itpro.nikkeibp.co.jp/article/COLUMN/20061019/251207/?ST=virtual&P=1

Intel社のサイトから引っ張ってきたIntel VT-dのわかりやすい図↓




なお、2009年現在、上記３つの技術はNehalemアーキテクチャのような最新CPUなら大体搭載されている。でも、コンシュマー向けだと搭載されていないのがあったり・・・・
・Intel Core i3とi5-750はVT-d技術未搭載（20090722-7）
http://www.virtualization.info/jp/2009/07/intel-core-i3i5-750vt-d20090722-7.html
自作erの人は買うとき注意だね


参考
・第１回　1985年以来のアーキテクチャを革新する新技術
http://itpro.nikkeibp.co.jp/article/COLUMN/20061010/250154/?ST=virtual&P=1
・仮想化技術を学ぶ
http://itpro.nikkeibp.co.jp/article/lecture/20061228/258010/?ST=lecture&P=3
・仮想化における EPT と VT-d の効果
http://agile-cat-mits.spaces.live.com/blog/cns!684876E6A5CF0AF4!323.entry
・Intel社がネステドページテーブルとI/O仮想化に対応した新CPUとNICをリリース（20090331-1）
http://www.virtualization.info/jp/2009/03/inteliocpunic20090331-1.html
・Ciittrriix XenSerrverr参考資料（PDF）
http://www.viops.jp/viops03-citrix-20090529.pdf
・特集「仮想化の正体」（２） Part2 コンピュータ／「分割」を極め，「結合」の段階に
http://itpro.nikkeibp.co.jp/article/COLUMN/20051125/225202/
・サーバ仮想化機構「Virtage」 I/O 仮想化支援機構
http://www.hitachi.co.jp/products/bladesymphony/virtual/dl/virtage_wp04.pdf

□ホスト型とハイパーバイザ型
仮想化ソフトには大きく分類してホスト型とハイパーバイザ型の2種類がある。
●ホスト型
・代表的なソフト
VMware Workstation/Player、VMware Server、VMware Fusion、Virtual PC、VirtualBox、QEMU
・仮想OSからのデバイスへのアクセス方法
ホストOS上のVMMを経由し、ホストOSを介して処理を行う
・性能
ホストOS上のVMMで処理が行われるため、命令変換のオーバヘッドが大きい
・デバイスドライバ
「実在するハードウェア」をエミュレートして仮想環境を実現するため、（そのハードウェア上で動くOSなら）基本的にどんなOSでも動く。エミュレーションにはたいてい古いハードウェアを用いているため、デバイスドライバはたいていOSの標準ドライバで事足りる。
・CPUリソースのパーティショニング
ホストOS上のCPUリソースをゲストOSに割り振るため、ホストOSとゲストOSのCPU使用率が互いに影響しやすい

●ハイパーバイザ型
・代表的なソフト
VMWare ESX/ESXi、Xen、Hyper-V、KVM
・仮想OSからのデバイスへのアクセス方法
ハードウェアとOSの間のレイヤに存在するハイパーバイザと呼ばれるVMMを介して処理を行う
※Xenでは、ホストOSに相当するドメイン0のドライバを経由することによって、ゲストOSに相当するドメインUからのデバイスアクセスが行われる。
※Hyper-Vでは、ホストOSに相当するペアレントパーティションのドライバを経由することによって、ゲストOSに相当するチャイルドパーティションからのデバイスアクセスが行われる。
・性能
ハイパーバイザ上のVMMで処理が行われるため、命令変換のオーバヘッドが小さい
・デバイスドライバ
VMWare ESX/ESXiの場合、ハイパーバイザにハードウェアを制御する専用のデバイスドライバが必要なため、使用できるデバイスに限りがある。Xen、Hyper-Vは、ホスト型仮想化ソフトと似たようなドライバの使い方をするため、基本的にはOS標準ドライバで事足りる。
・CPUリソースのパーティショニング
ハイパーバイザ上でCPUリソースが論理的に区分けされるため、仮想マシン同士のCPU使用率の影響が少ない
（これは自分の推測ではあるが、ハイパーバイザでCPUリソースが綺麗にパーティショニングされることによって、コンテキストスイッチによるオーバヘッドもホスト型に比べ少なくなっているのではないかと考えている。プロセッサアフィニティのように、割り当てたCPUのスレッドをずっと保持したまま使用するようになるので。）

※Xen、Hyper-V、KVMは場合によっては（ホストとハイパーバイザの）ハイブリッド型として分類されることもある

参考
・【仮想化の教室　第4回】 仮想化の進化 ～ハイパーバイザとは？～ 
http://www.computerworld.jp/topics/mws/155890.html
・仮想化はうさんくさい？ 仕組みから仮想化を理解する
http://enterprise.watch.impress.co.jp/cda/virtual/2009/04/13/15256.html


□完全仮想化（Full Virtualization）と準仮想化（Para Virtualization）
完全仮想化＝ホスト型仮想化ソフト
準仮想化＝ハイパーバイザ型仮想化ソフト
と勘違いしていた・・・・。
あくまで違いは、使用する仮想OS。準仮想化のほうが、仮想化用にカーネルやドライバが最適化されているため高い性能がでる。ただし、ドライバの改変も準仮想化に入れてしまっているのは俺の考えなので、定義が間違ってたらごめんなさい。

●完全仮想化
・概要
実機でも使用する一般のOSを、改変なしにそのまま使用する方式
・OS
全て。Windows系OSは全てこちらの方式（ただし、デバイスドライバレベルではWindowsも準仮想化に対応したものがある）
・デバイスドライバ
OS搭載標準ドライバ

●準仮想化
・概要
仮想環境で動かすことを前提に改変されたOSを使用する方式
・OS
仮想化用にカーネルが最適化されているLinux系OS
・デバイスドライバ
仮想化用に最適化されたドライバ
（VMWareのVMWaretoolsやHyper-Vの統合パッケージ等も含む）

参考
・【仮想化の教室　第6回】 デバイスの仮想化 ～準仮想化デバイスと完全仮想化デバイス～ 
http://www.computerworld.jp/topics/mws/157229.html
・【仮想化の教室　第7回】 仮想化のコラボ ～Xenの準仮想化仮想マシンをHyper-Vで動かす～ 
http://www.computerworld.jp/topics/mws/157869.html
・記事:完全仮想化されたゲストOSのI/O性能を向上させるpara-virtualized driver
http://www.jp.redhat.com/magazine/jp/200806/rhel.html
